{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 로딩\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 준비\n",
    "\n",
    "data1 = pd.read_excel(\"2조 괴담 파일.xlsx\", sheet_name='BJY', header= 1)\n",
    "data2 = pd.read_excel(\"2조 괴담 파일.xlsx\", sheet_name='LHG_Neapolitan', header= 1)\n",
    "data3 = pd.read_excel(\"2조 괴담 파일.xlsx\", sheet_name= 'LHG_Horror', header= 1)\n",
    "data4 = pd.read_excel(\"2조 괴담 파일.xlsx\", sheet_name= 'SY', header = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CLASS</th>\n",
       "      <th>TYPE</th>\n",
       "      <th>FROM</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H</td>\n",
       "      <td>E</td>\n",
       "      <td>K</td>\n",
       "      <td>\"그런데 어떻게 알았어?\" 이렇게 묻더라구요, 저는 뭐가? 라고 대답하니 \"꿈인줄 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H</td>\n",
       "      <td>E</td>\n",
       "      <td>K</td>\n",
       "      <td>또 똑같은 꿈을 꿨습니다. 방에서 일어나니까 옆방에서 여자가 미친듯이 웃으면서 00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H</td>\n",
       "      <td>E</td>\n",
       "      <td>K</td>\n",
       "      <td>그리고 출근 한 다음 근무 후 새벽에 퇴근하고 아침에 잠들었는데 낮 열두시쯤 까지 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H</td>\n",
       "      <td>E</td>\n",
       "      <td>K</td>\n",
       "      <td>여튼 때는 20대 중반이었나?? 여름이었던것 같은데 여느때처럼 내방에서 잠을 자고 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H</td>\n",
       "      <td>E</td>\n",
       "      <td>K</td>\n",
       "      <td>온몸에 소름이 돋으면서 식은땀 ㅈㄴ나고 뭐지?? 도둑인가?? 이런 생각에 너무 무서...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CLASS TYPE FROM                                               TEXT\n",
       "0     H    E    K  \"그런데 어떻게 알았어?\" 이렇게 묻더라구요, 저는 뭐가? 라고 대답하니 \"꿈인줄 ...\n",
       "1     H    E    K  또 똑같은 꿈을 꿨습니다. 방에서 일어나니까 옆방에서 여자가 미친듯이 웃으면서 00...\n",
       "2     H    E    K  그리고 출근 한 다음 근무 후 새벽에 퇴근하고 아침에 잠들었는데 낮 열두시쯤 까지 ...\n",
       "3     H    E    K  여튼 때는 20대 중반이었나?? 여름이었던것 같은데 여느때처럼 내방에서 잠을 자고 ...\n",
       "4     H    E    K  온몸에 소름이 돋으면서 식은땀 ㅈㄴ나고 뭐지?? 도둑인가?? 이런 생각에 너무 무서..."
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 괴담 데이터 하나의 데이터프레임으로 합치기\n",
    "\n",
    "horror = pd.concat([data1, data2, data3, data4], axis = 0)\n",
    "horror.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TYPE</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E</td>\n",
       "      <td>\"그런데 어떻게 알았어?\" 이렇게 묻더라구요, 저는 뭐가? 라고 대답하니 \"꿈인줄 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E</td>\n",
       "      <td>또 똑같은 꿈을 꿨습니다. 방에서 일어나니까 옆방에서 여자가 미친듯이 웃으면서 00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E</td>\n",
       "      <td>그리고 출근 한 다음 근무 후 새벽에 퇴근하고 아침에 잠들었는데 낮 열두시쯤 까지 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E</td>\n",
       "      <td>여튼 때는 20대 중반이었나?? 여름이었던것 같은데 여느때처럼 내방에서 잠을 자고 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E</td>\n",
       "      <td>온몸에 소름이 돋으면서 식은땀 ㅈㄴ나고 뭐지?? 도둑인가?? 이런 생각에 너무 무서...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  TYPE                                               TEXT\n",
       "0    E  \"그런데 어떻게 알았어?\" 이렇게 묻더라구요, 저는 뭐가? 라고 대답하니 \"꿈인줄 ...\n",
       "1    E  또 똑같은 꿈을 꿨습니다. 방에서 일어나니까 옆방에서 여자가 미친듯이 웃으면서 00...\n",
       "2    E  그리고 출근 한 다음 근무 후 새벽에 퇴근하고 아침에 잠들었는데 낮 열두시쯤 까지 ...\n",
       "3    E  여튼 때는 20대 중반이었나?? 여름이었던것 같은데 여느때처럼 내방에서 잠을 자고 ...\n",
       "4    E  온몸에 소름이 돋으면서 식은땀 ㅈㄴ나고 뭐지?? 도둑인가?? 이런 생각에 너무 무서..."
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#나는 type만 사용할 거니까 나머지는 제거\n",
    "horror = horror.drop(['CLASS', 'FROM'], axis = 1)\n",
    "\n",
    "horror.head() #없어져땅. 굿~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 전처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "punc = string.punctuation\n",
    "\n",
    "# 정규표현식 패턴만들기\n",
    "hangle_pattern = \"[^ㄱ-ㅎㅏ-ㅣ가-힣]\"\n",
    "num_pattern = \"[0-9]\"\n",
    "eng_pattern = \"[a-zA-Z]\"\n",
    "\n",
    "# 한글, 숫자, 영어만 남기기\n",
    "horror['TEXT'] = horror['TEXT'].str.replace(hangle_pattern, '', regex=True)\n",
    "horror['TEXT'] = horror['TEXT'].str.replace(num_pattern, '', regex=True)\n",
    "horror['TEXT'] = horror['TEXT'].str.replace(eng_pattern, '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈 문장 있으면 제거\n",
    "import numpy as np\n",
    "horror['TEXT'].replace('', np.nan, inplace=True)\n",
    "horror = horror.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#불용어 사전 불러오기\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "filename ='../datas/kr_stopwords.txt'\n",
    "url = \"https://gist.githubusercontent.com/chulgil/d10b18575a73778da4bc83853385465c/raw/a1a451421097fa9a93179cb1f1f0dc392f1f9da9/stopwords.txt\"\n",
    "ret = urlretrieve(url, filename)\n",
    "stopwords = set(open(filename).read().splitlines())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#토크나이저\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분할하기(train, test)\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(horror, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TYPE    0\n",
       "TEXT    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isin(stopwords).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2560, 2), (640, 2))"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "['관리', '실', '에서', '여성', '직원', '이', '당신', '의', '휴대폰', '으로', '어디', '로', '와', '달라', '는', '등', '의', '요청', '이', '왔다면', '바로', '끊으십시오', '관리', '실', '에선', '어떤', '경우', '에서도', '당신', '의', '휴대폰', '에', '직접', '전화', '를', '하지', '않을것이며', '관리', '실', '에서는', '여성', '직원', '을', '고용', '하', '지', '않습니다']\n"
     ]
    }
   ],
   "source": [
    "for sentence in train['TEXT']:\n",
    "    print(type(sentence))\n",
    "    print(okt.morphs(sentence))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[167], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 토크나이즈 이후 불용어 제거하기\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_tokens \u001b[38;5;241m=\u001b[39m [okt\u001b[38;5;241m.\u001b[39mmorphs(sentence)\u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTEXT\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;66;03m# tokenize\u001b[39;00m\n\u001b[0;32m      3\u001b[0m train_tokens \u001b[38;5;241m=\u001b[39m [[token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m sent \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopwords]\u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m train_tokens] \u001b[38;5;66;03m# extract stopwords\u001b[39;00m\n\u001b[0;32m      4\u001b[0m test_tokens \u001b[38;5;241m=\u001b[39m [okt\u001b[38;5;241m.\u001b[39mmorphs(sentence)\u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTEXT\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;66;03m#tokenize\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[167], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 토크나이즈 이후 불용어 제거하기\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_tokens \u001b[38;5;241m=\u001b[39m [\u001b[43mokt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmorphs\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTEXT\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;66;03m# tokenize\u001b[39;00m\n\u001b[0;32m      3\u001b[0m train_tokens \u001b[38;5;241m=\u001b[39m [[token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m sent \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopwords]\u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m train_tokens] \u001b[38;5;66;03m# extract stopwords\u001b[39;00m\n\u001b[0;32m      4\u001b[0m test_tokens \u001b[38;5;241m=\u001b[39m [okt\u001b[38;5;241m.\u001b[39mmorphs(sentence)\u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTEXT\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;66;03m#tokenize\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kdp\\anaconda3\\envs\\NLP_38\\lib\\site-packages\\konlpy\\tag\\_okt.py:89\u001b[0m, in \u001b[0;36mOkt.morphs\u001b[1;34m(self, phrase, norm, stem)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmorphs\u001b[39m(\u001b[38;5;28mself\u001b[39m, phrase, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, stem\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     87\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Parse phrase to morphemes.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [s \u001b[38;5;28;01mfor\u001b[39;00m s, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphrase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstem\u001b[49m\u001b[43m)\u001b[49m]\n",
      "File \u001b[1;32mc:\\Users\\kdp\\anaconda3\\envs\\NLP_38\\lib\\site-packages\\konlpy\\tag\\_okt.py:71\u001b[0m, in \u001b[0;36mOkt.pos\u001b[1;34m(self, phrase, norm, stem, join)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"POS tagger.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03mIn contrast to other classes in this subpackage,\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03mthis POS tagger doesn't have a `flatten` option,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m:param join: If True, returns joined sets of morph and tag.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     69\u001b[0m validate_phrase_inputs(phrase)\n\u001b[1;32m---> 71\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjki\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mphrase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mjpype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlang\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBoolean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mjpype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlang\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBoolean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstem\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtoArray()\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m join:\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tokens]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 토크나이즈 이후 불용어 제거하기\n",
    "train_tokens = [okt.morphs(sentence)for sentence in train['TEXT']] # tokenize\n",
    "train_tokens = [[token for token in sent if token not in stopwords]for sent in train_tokens] # extract stopwords\n",
    "test_tokens = [okt.morphs(sentence)for sentence in test['TEXT']] #tokenize\n",
    "test_tokens = [[token for token in sent if token not in stopwords]for sent in test_tokens] #extract stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_train = train_tokens\n",
    "backup_test = test_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['할아버지',\n",
       " '돌아가신',\n",
       " '후난',\n",
       " '추억',\n",
       " '떠올리며',\n",
       " '유품',\n",
       " '정리',\n",
       " '하고',\n",
       " '있었',\n",
       " '다그',\n",
       " '오래된',\n",
       " '포르노',\n",
       " '컬렉션',\n",
       " '살펴보던난',\n",
       " '내게',\n",
       " '여동생',\n",
       " '있었다는',\n",
       " '사실',\n",
       " '깨닫게',\n",
       " '되었다']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_tokens = backup_train\n",
    "# test_tokens = backup_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어사전 생성하기\n",
    "vocab = {'<PAD>': 8000, '<UNK>':7999}\n",
    "for sen in train_tokens:\n",
    "    for word in sen:\n",
    "        if word in vocab:\n",
    "            vocab[word] += 1\n",
    "        else:\n",
    "            vocab[word] = 1\n",
    "        \n",
    "# 집합으로 변환하여 패딩 값 추가 \n",
    "\n",
    "# vocab2 = {}\n",
    "# for sen in test_tokens:\n",
    "#     for word in sen:\n",
    "#         if word is not None:  # None이 아닌 경우에만 처리\n",
    "#             if word in vocab:\n",
    "#                 vocab2[word] += 1\n",
    "#             else:\n",
    "#                 vocab2[word] = 1\n",
    "\n",
    "# 결과 확인\n",
    "# 패드가 없어서 계속 none 값이 섞이길래 강제로 추가했다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<PAD>', 8000),\n",
       " ('<UNK>', 7999),\n",
       " ('는', 1919),\n",
       " ('은', 1629),\n",
       " ('한', 1121),\n",
       " ('도', 714),\n",
       " ('내', 541),\n",
       " ('다', 513),\n",
       " ('말', 490),\n",
       " ('하고', 456)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(vocab.items(), key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<PAD>', 8000),\n",
       " ('<UNK>', 7999),\n",
       " ('는', 1919),\n",
       " ('은', 1629),\n",
       " ('한', 1121),\n",
       " ('도', 714),\n",
       " ('내', 541),\n",
       " ('다', 513),\n",
       " ('말', 490),\n",
       " ('하고', 456)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 불용어 처리 한번 더,,,,,,,,,,\n",
    "clean = {token: value for token,value in vocab.items() if token not in stopwords}\n",
    "\n",
    "sorted(clean.items(), key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코딩 / 디코딩 딕셔너리 생성하기\n",
    "\n",
    "encode = {token: idx for idx, token in enumerate(vocab)}\n",
    "decode = {idx: token for idx, token in enumerate(vocab)}\n",
    "\n",
    "UNK = encode.get('<UNK>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 인코딩하기\n",
    "\n",
    "train_id = [[encode.get(token, UNK) for token in sen]for sen in train_tokens]\n",
    "test_id = [[encode.get(token,UNK) for token in sen] for sen in test_tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2560, 640)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_id), len(test_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[720,\n",
       " 722,\n",
       " 1,\n",
       " 9105,\n",
       " 1,\n",
       " 2132,\n",
       " 1840,\n",
       " 435,\n",
       " 3851,\n",
       " 283,\n",
       " 6905,\n",
       " 1,\n",
       " 4173,\n",
       " 1,\n",
       " 858,\n",
       " 2057,\n",
       " 5503,\n",
       " 941,\n",
       " 3824,\n",
       " 244]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PAD\n",
    "\n",
    "def pad_sequences(sequences, max_len, pad_token):\n",
    "    padded = []\n",
    "    for seq in sequences:\n",
    "        if seq is None:  # 시퀀스가 None일 경우 패딩하지 않고 건너뜁니다.\n",
    "            continue\n",
    "        if len(seq) < max_len:\n",
    "            seq = seq + [pad_token] * (max_len - len(seq))\n",
    "        else:\n",
    "            seq = seq[:max_len]\n",
    "        padded.append(seq)\n",
    "    return padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN_ID: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 2, 3, 12, 13, 14, 6, 15, 16, 17, 18, 2, 3, 19, 4, 5, 20, 21, 22, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "\n",
      " TEST_ID: [720, 722, 1, 9105, 1, 2132, 1840, 435, 3851, 283, 6905, 1, 4173, 1, 858, 2057, 5503, 941, 3824, 244, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = max([len(sen) for sen in train_id + test_id])\n",
    "PAD_ID = encode.get('<PAD>')\n",
    "\n",
    "train_id = pad_sequences(train_id, MAX_LEN, PAD_ID)\n",
    "test_id = pad_sequences(test_id, MAX_LEN, PAD_ID)\n",
    "\n",
    "print(f'TRAIN_ID: {train_id[0]},\\n\\n TEST_ID: {test_id[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN 모델 구축하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 train, test의 label과 text 나누기\n",
    "xtrain = np.array(train_id)\n",
    "ytrain = np.array(train['TYPE'])\n",
    "xtest = np.array(test_id)\n",
    "ytest = np.array(test['TYPE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  720,   722,     1, ...,     0,     0,     0],\n",
       "       [  867, 20207,   344, ...,     0,     0,     0],\n",
       "       [  510,  7710,  8296, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [    1,  1248,  1818, ...,     0,     0,     0],\n",
       "       [ 5707,     1, 15602, ...,     0,     0,     0],\n",
       "       [ 6533,  1513,     9, ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['G', 'G', 'E', ..., 'G', 'G', 'E'], dtype=object)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨이 정수가 아니라서 계속 오류가 생긴다. 제기랄.\n",
    "# 라벨인코더도 안 먹어서 내가 그냥 바꿔주도록 하자.\n",
    "label_to_index = {'G': 0, 'E': 1}\n",
    "\n",
    "# ytrain 인코딩\n",
    "y_train = [label_to_index[label] for label in train['TYPE']]\n",
    "y_train = torch.FloatTensor(y_train)\n",
    "\n",
    "# ytest 인코딩\n",
    "y_test = [label_to_index[label] for label in test['TYPE']]\n",
    "y_test = torch.FloatTensor(y_test)\n",
    "\n",
    "xtrain = torch.LongTensor(xtrain)\n",
    "xtest = torch.LongTensor(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23943"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어사전 크기 계산하기\n",
    "vocab_num = len(encode) + 1 #패딩 크기(1) 계산하기\n",
    "\n",
    "vocab_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN 모델 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RNN\n",
    "\n",
    "# class RNNClassifier(nn.Module):\n",
    "#     def __init__(self, n_vocab, output_size, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "#         super().__init__()\n",
    "#         self.embedding = nn.Embedding(n_vocab, embedding_dim)\n",
    "#         self.rnn = nn.RNN(embedding_dim, hidden_dim, n_layers, batch_first=True, dropout=dropout)\n",
    "#         self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         embedded = self.embedding(x)\n",
    "#         output, hidden = self.rnn(embedded)\n",
    "#         output = self.fc(output[:,-1,:])\n",
    "#         return output\n",
    "        \n",
    "# model = RNNClassifier(vocab_num, output_size=2, embedding_dim=64, hidden_dim=128, n_layers=2, dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, n_vocab, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_vocab, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, n_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)  # 출력 크기를 1로 변경 (이진 분류 문제)\n",
    "        self.sigmoid = nn.Sigmoid()  # 시그모이드 함수\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        output = self.sigmoid(output)  # 시그모이드 함수 적용\n",
    "        return output\n",
    "\n",
    "# 모델 생성\n",
    "model = RNNClassifier(vocab_num, embedding_dim=64, hidden_dim=128, n_layers=2, dropout=0.5)\n",
    "\n",
    "# 손실함수 및 옵티마이저 정의\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수, 옵티마이저 정의하기\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 1.,  ..., 0., 0., 1.])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain.dtype\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# 데이터셋 및 데이터 로더 생성\n",
    "train_dataset = TensorDataset(xtrain, y_train)\n",
    "# ytest와 동일한 개수의 샘플을 xtest에서 선택하여 새로운 xtest_subset을 만듭니다.\n",
    "xtest_subset = xtest[:len(y_test)]\n",
    "test_dataset = TensorDataset(xtest_subset, y_test)\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "640"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2560, 288]), (2560,), torch.Size([640, 288]), (640,))"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain.shape, ytrain.shape, xtest_subset.shape, ytest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 720,  722,    1, 9105,    1, 2132, 1840,  435, 3851,  283, 6905,    1,\n",
       "         4173,    1,  858, 2057, 5503,  941, 3824,  244,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0]),\n",
       " 'G')"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtest[0], ytest[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([288]) torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "for f, t in train_dataset:\n",
    "    print(f.shape, t.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.6828194856643677, Accuracy: 53.125%\n",
      "Epoch: 2, Loss: 0.6843532919883728, Accuracy: 53.125%\n",
      "Epoch: 3, Loss: 0.6832965612411499, Accuracy: 53.125%\n",
      "Epoch: 4, Loss: 0.6832455992698669, Accuracy: 53.125%\n",
      "Epoch: 5, Loss: 0.6837669610977173, Accuracy: 53.125%\n",
      "Epoch: 6, Loss: 0.683853268623352, Accuracy: 53.125%\n",
      "Epoch: 7, Loss: 0.6840257048606873, Accuracy: 53.125%\n",
      "Epoch: 8, Loss: 0.683338463306427, Accuracy: 53.125%\n",
      "Epoch: 9, Loss: 0.6823488473892212, Accuracy: 53.125%\n",
      "Epoch: 10, Loss: 0.6820040941238403, Accuracy: 53.125%\n",
      "Epoch: 11, Loss: 0.6842759847640991, Accuracy: 53.125%\n",
      "Epoch: 12, Loss: 0.6837902069091797, Accuracy: 53.125%\n",
      "Epoch: 13, Loss: 0.6830954551696777, Accuracy: 53.125%\n",
      "Epoch: 14, Loss: 0.6827939748764038, Accuracy: 53.125%\n",
      "Epoch: 15, Loss: 0.6828820109367371, Accuracy: 53.125%\n",
      "Epoch: 16, Loss: 0.6841403245925903, Accuracy: 53.125%\n",
      "Epoch: 17, Loss: 0.6836097240447998, Accuracy: 53.125%\n",
      "Epoch: 18, Loss: 0.6823874711990356, Accuracy: 53.125%\n",
      "Epoch: 19, Loss: 0.6828668713569641, Accuracy: 53.125%\n",
      "Epoch: 20, Loss: 0.6826601028442383, Accuracy: 53.125%\n",
      "Epoch: 21, Loss: 0.6823596954345703, Accuracy: 53.125%\n",
      "Epoch: 22, Loss: 0.6827507615089417, Accuracy: 53.125%\n",
      "Epoch: 23, Loss: 0.6831845641136169, Accuracy: 53.125%\n",
      "Epoch: 24, Loss: 0.682557225227356, Accuracy: 53.125%\n",
      "Epoch: 25, Loss: 0.6834293007850647, Accuracy: 53.125%\n",
      "Epoch: 26, Loss: 0.6821638345718384, Accuracy: 53.125%\n",
      "Epoch: 27, Loss: 0.6830233335494995, Accuracy: 53.125%\n",
      "Epoch: 28, Loss: 0.6827390789985657, Accuracy: 53.125%\n",
      "Epoch: 29, Loss: 0.6827813386917114, Accuracy: 53.125%\n",
      "Epoch: 30, Loss: 0.6832761168479919, Accuracy: 53.125%\n",
      "Epoch: 31, Loss: 0.6827982068061829, Accuracy: 53.125%\n",
      "Epoch: 32, Loss: 0.6825768351554871, Accuracy: 53.125%\n",
      "Epoch: 33, Loss: 0.6829919815063477, Accuracy: 53.125%\n",
      "Epoch: 34, Loss: 0.6824833750724792, Accuracy: 53.125%\n",
      "Epoch: 35, Loss: 0.6832424998283386, Accuracy: 53.125%\n",
      "Epoch: 36, Loss: 0.6818268299102783, Accuracy: 53.125%\n",
      "Epoch: 37, Loss: 0.6824793815612793, Accuracy: 53.125%\n",
      "Epoch: 38, Loss: 0.6821107864379883, Accuracy: 53.125%\n",
      "Epoch: 39, Loss: 0.6833702325820923, Accuracy: 53.125%\n",
      "Epoch: 40, Loss: 0.6831712126731873, Accuracy: 53.125%\n",
      "Epoch: 41, Loss: 0.6831583380699158, Accuracy: 53.125%\n",
      "Epoch: 42, Loss: 0.6838274002075195, Accuracy: 53.125%\n",
      "Epoch: 43, Loss: 0.6817499995231628, Accuracy: 53.125%\n",
      "Epoch: 44, Loss: 0.6824632883071899, Accuracy: 53.125%\n",
      "Epoch: 45, Loss: 0.6828857660293579, Accuracy: 53.125%\n",
      "Epoch: 46, Loss: 0.6832233667373657, Accuracy: 53.125%\n",
      "Epoch: 47, Loss: 0.6827434301376343, Accuracy: 53.125%\n",
      "Epoch: 48, Loss: 0.682883620262146, Accuracy: 53.125%\n",
      "Epoch: 49, Loss: 0.6822265386581421, Accuracy: 53.125%\n",
      "Epoch: 50, Loss: 0.6831937432289124, Accuracy: 53.125%\n",
      "Epoch: 51, Loss: 0.6829207539558411, Accuracy: 53.125%\n",
      "Epoch: 52, Loss: 0.6826589107513428, Accuracy: 53.125%\n",
      "Epoch: 53, Loss: 0.6841219663619995, Accuracy: 53.125%\n",
      "Epoch: 54, Loss: 0.6830159425735474, Accuracy: 53.125%\n",
      "Epoch: 55, Loss: 0.6832460761070251, Accuracy: 53.125%\n",
      "Epoch: 56, Loss: 0.6828972101211548, Accuracy: 53.125%\n",
      "Epoch: 57, Loss: 0.6828112602233887, Accuracy: 53.125%\n",
      "Epoch: 58, Loss: 0.6828681826591492, Accuracy: 53.125%\n",
      "Epoch: 59, Loss: 0.6826525330543518, Accuracy: 53.125%\n",
      "Epoch: 60, Loss: 0.6818698644638062, Accuracy: 53.125%\n",
      "Epoch: 61, Loss: 0.6829472780227661, Accuracy: 53.125%\n",
      "Epoch: 62, Loss: 0.6824808120727539, Accuracy: 53.125%\n",
      "Epoch: 63, Loss: 0.6815623044967651, Accuracy: 53.125%\n",
      "Epoch: 64, Loss: 0.681855320930481, Accuracy: 53.125%\n",
      "Epoch: 65, Loss: 0.6814215779304504, Accuracy: 53.125%\n",
      "Epoch: 66, Loss: 0.683066725730896, Accuracy: 53.125%\n",
      "Epoch: 67, Loss: 0.6830121874809265, Accuracy: 53.125%\n",
      "Epoch: 68, Loss: 0.6822632551193237, Accuracy: 53.125%\n",
      "Epoch: 69, Loss: 0.6832099556922913, Accuracy: 53.125%\n",
      "Epoch: 70, Loss: 0.6829405426979065, Accuracy: 53.125%\n",
      "Epoch: 71, Loss: 0.682374894618988, Accuracy: 53.125%\n",
      "Epoch: 72, Loss: 0.6823015213012695, Accuracy: 53.125%\n",
      "Epoch: 73, Loss: 0.6834633350372314, Accuracy: 53.125%\n",
      "Epoch: 74, Loss: 0.6827394366264343, Accuracy: 53.125%\n",
      "Epoch: 75, Loss: 0.6823296546936035, Accuracy: 53.125%\n",
      "Epoch: 76, Loss: 0.6822460293769836, Accuracy: 53.125%\n",
      "Epoch: 77, Loss: 0.6834291815757751, Accuracy: 53.125%\n",
      "Epoch: 78, Loss: 0.6830257177352905, Accuracy: 53.125%\n",
      "Epoch: 79, Loss: 0.6838812828063965, Accuracy: 53.125%\n",
      "Epoch: 80, Loss: 0.6827341318130493, Accuracy: 53.125%\n",
      "Epoch: 81, Loss: 0.6833117008209229, Accuracy: 53.125%\n",
      "Epoch: 82, Loss: 0.6831787824630737, Accuracy: 53.125%\n",
      "Epoch: 83, Loss: 0.6838327646255493, Accuracy: 53.125%\n",
      "Epoch: 84, Loss: 0.682653546333313, Accuracy: 53.125%\n",
      "Epoch: 85, Loss: 0.6826480627059937, Accuracy: 53.125%\n",
      "Epoch: 86, Loss: 0.6821845769882202, Accuracy: 53.125%\n",
      "Epoch: 87, Loss: 0.683371901512146, Accuracy: 53.125%\n",
      "Epoch: 88, Loss: 0.6828590631484985, Accuracy: 53.125%\n",
      "Epoch: 89, Loss: 0.6825498342514038, Accuracy: 53.125%\n",
      "Epoch: 90, Loss: 0.6830950975418091, Accuracy: 53.125%\n",
      "Epoch: 91, Loss: 0.6822390556335449, Accuracy: 53.125%\n",
      "Epoch: 92, Loss: 0.6833440065383911, Accuracy: 53.125%\n",
      "Epoch: 93, Loss: 0.6834825277328491, Accuracy: 53.125%\n",
      "Epoch: 94, Loss: 0.6832535862922668, Accuracy: 53.125%\n",
      "Epoch: 95, Loss: 0.6834372878074646, Accuracy: 53.125%\n",
      "Epoch: 96, Loss: 0.6820715665817261, Accuracy: 53.125%\n",
      "Epoch: 97, Loss: 0.6832352876663208, Accuracy: 53.125%\n",
      "Epoch: 98, Loss: 0.6820651292800903, Accuracy: 53.125%\n",
      "Epoch: 99, Loss: 0.6828699111938477, Accuracy: 53.125%\n",
      "Epoch: 100, Loss: 0.6827868223190308, Accuracy: 53.125%\n",
      "Epoch: 101, Loss: 0.6826173663139343, Accuracy: 53.125%\n",
      "Epoch: 102, Loss: 0.6821830868721008, Accuracy: 53.125%\n",
      "Epoch: 103, Loss: 0.6824747323989868, Accuracy: 53.125%\n",
      "Epoch: 104, Loss: 0.6818515062332153, Accuracy: 53.125%\n",
      "Epoch: 105, Loss: 0.6830228567123413, Accuracy: 53.125%\n",
      "Epoch: 106, Loss: 0.6830068230628967, Accuracy: 53.125%\n",
      "Epoch: 107, Loss: 0.6828675866127014, Accuracy: 53.125%\n",
      "Epoch: 108, Loss: 0.68262779712677, Accuracy: 53.125%\n",
      "Epoch: 109, Loss: 0.6827124357223511, Accuracy: 53.125%\n",
      "Epoch: 110, Loss: 0.6830533742904663, Accuracy: 53.125%\n",
      "Epoch: 111, Loss: 0.6821861267089844, Accuracy: 53.125%\n",
      "Epoch: 112, Loss: 0.6823040246963501, Accuracy: 53.125%\n",
      "Epoch: 113, Loss: 0.6829585433006287, Accuracy: 53.125%\n",
      "Epoch: 114, Loss: 0.6828869581222534, Accuracy: 53.125%\n",
      "Epoch: 115, Loss: 0.6826679110527039, Accuracy: 53.125%\n",
      "Epoch: 116, Loss: 0.6822234988212585, Accuracy: 53.125%\n",
      "Epoch: 117, Loss: 0.6823776364326477, Accuracy: 53.125%\n",
      "Epoch: 118, Loss: 0.6835137605667114, Accuracy: 53.125%\n",
      "Epoch: 119, Loss: 0.6836237907409668, Accuracy: 53.125%\n",
      "Epoch: 120, Loss: 0.6829351186752319, Accuracy: 53.125%\n",
      "Epoch: 121, Loss: 0.6830164194107056, Accuracy: 53.125%\n",
      "Epoch: 122, Loss: 0.6822012066841125, Accuracy: 53.125%\n",
      "Epoch: 123, Loss: 0.6827954053878784, Accuracy: 53.125%\n",
      "Epoch: 124, Loss: 0.6821518540382385, Accuracy: 53.125%\n",
      "Epoch: 125, Loss: 0.6822551488876343, Accuracy: 53.125%\n",
      "Epoch: 126, Loss: 0.682713508605957, Accuracy: 53.125%\n",
      "Epoch: 127, Loss: 0.6830275654792786, Accuracy: 53.125%\n",
      "Epoch: 128, Loss: 0.6822172403335571, Accuracy: 53.125%\n",
      "Epoch: 129, Loss: 0.6831251978874207, Accuracy: 53.125%\n",
      "Epoch: 130, Loss: 0.6829410791397095, Accuracy: 53.125%\n",
      "Epoch: 131, Loss: 0.6818939447402954, Accuracy: 53.125%\n",
      "Epoch: 132, Loss: 0.6822673678398132, Accuracy: 53.125%\n",
      "Epoch: 133, Loss: 0.6826328635215759, Accuracy: 53.125%\n",
      "Epoch: 134, Loss: 0.6824852228164673, Accuracy: 53.125%\n",
      "Epoch: 135, Loss: 0.6827844381332397, Accuracy: 53.125%\n",
      "Epoch: 136, Loss: 0.6822317242622375, Accuracy: 53.125%\n",
      "Epoch: 137, Loss: 0.6833990812301636, Accuracy: 53.125%\n",
      "Epoch: 138, Loss: 0.6824072599411011, Accuracy: 53.125%\n",
      "Epoch: 139, Loss: 0.6814013123512268, Accuracy: 53.125%\n",
      "Epoch: 140, Loss: 0.6828827857971191, Accuracy: 53.125%\n",
      "Epoch: 141, Loss: 0.68206787109375, Accuracy: 53.125%\n",
      "Epoch: 142, Loss: 0.6829549074172974, Accuracy: 53.125%\n",
      "Epoch: 143, Loss: 0.682212233543396, Accuracy: 53.125%\n",
      "Epoch: 144, Loss: 0.6829489469528198, Accuracy: 53.125%\n",
      "Epoch: 145, Loss: 0.6821758151054382, Accuracy: 53.125%\n",
      "Epoch: 146, Loss: 0.6824612617492676, Accuracy: 53.125%\n",
      "Epoch: 147, Loss: 0.6833619475364685, Accuracy: 53.125%\n",
      "Epoch: 148, Loss: 0.6823858618736267, Accuracy: 53.125%\n",
      "Epoch: 149, Loss: 0.6829036474227905, Accuracy: 53.125%\n",
      "Epoch: 150, Loss: 0.6832641363143921, Accuracy: 53.125%\n",
      "Epoch: 151, Loss: 0.6823166608810425, Accuracy: 53.125%\n",
      "Epoch: 152, Loss: 0.681975245475769, Accuracy: 53.125%\n",
      "Epoch: 153, Loss: 0.6826568841934204, Accuracy: 53.125%\n",
      "Epoch: 154, Loss: 0.6822309494018555, Accuracy: 53.125%\n",
      "Epoch: 155, Loss: 0.6811568737030029, Accuracy: 53.125%\n",
      "Epoch: 156, Loss: 0.6827292442321777, Accuracy: 53.125%\n",
      "Epoch: 157, Loss: 0.6821707487106323, Accuracy: 53.125%\n",
      "Epoch: 158, Loss: 0.6824524402618408, Accuracy: 53.125%\n",
      "Epoch: 159, Loss: 0.6835941076278687, Accuracy: 53.125%\n",
      "Epoch: 160, Loss: 0.6830377578735352, Accuracy: 53.125%\n",
      "Epoch: 161, Loss: 0.6830093264579773, Accuracy: 53.125%\n",
      "Epoch: 162, Loss: 0.6833621859550476, Accuracy: 53.125%\n",
      "Epoch: 163, Loss: 0.6823452115058899, Accuracy: 53.125%\n",
      "Epoch: 164, Loss: 0.6823478937149048, Accuracy: 53.125%\n",
      "Epoch: 165, Loss: 0.6828861236572266, Accuracy: 53.125%\n",
      "Epoch: 166, Loss: 0.6845964789390564, Accuracy: 53.125%\n",
      "Epoch: 167, Loss: 0.6832200884819031, Accuracy: 53.125%\n",
      "Epoch: 168, Loss: 0.6831391453742981, Accuracy: 53.125%\n",
      "Epoch: 169, Loss: 0.682989239692688, Accuracy: 53.125%\n",
      "Epoch: 170, Loss: 0.6824017763137817, Accuracy: 53.125%\n",
      "Epoch: 171, Loss: 0.6828505396842957, Accuracy: 53.125%\n",
      "Epoch: 172, Loss: 0.6828330755233765, Accuracy: 53.125%\n",
      "Epoch: 173, Loss: 0.6827584505081177, Accuracy: 53.125%\n",
      "Epoch: 174, Loss: 0.6818131804466248, Accuracy: 53.125%\n",
      "Epoch: 175, Loss: 0.6826111674308777, Accuracy: 53.125%\n",
      "Epoch: 176, Loss: 0.6818361282348633, Accuracy: 53.125%\n",
      "Epoch: 177, Loss: 0.6826800107955933, Accuracy: 53.125%\n",
      "Epoch: 178, Loss: 0.6828006505966187, Accuracy: 53.125%\n",
      "Epoch: 179, Loss: 0.6831620335578918, Accuracy: 53.125%\n",
      "Epoch: 180, Loss: 0.6832101345062256, Accuracy: 53.125%\n",
      "Epoch: 181, Loss: 0.6833593249320984, Accuracy: 53.125%\n",
      "Epoch: 182, Loss: 0.6820996999740601, Accuracy: 53.125%\n",
      "Epoch: 183, Loss: 0.6822731494903564, Accuracy: 53.125%\n",
      "Epoch: 184, Loss: 0.682235836982727, Accuracy: 53.125%\n",
      "Epoch: 185, Loss: 0.6818938851356506, Accuracy: 53.125%\n",
      "Epoch: 186, Loss: 0.6823701858520508, Accuracy: 53.125%\n",
      "Epoch: 187, Loss: 0.681782066822052, Accuracy: 53.125%\n",
      "Epoch: 188, Loss: 0.6825579404830933, Accuracy: 53.125%\n",
      "Epoch: 189, Loss: 0.6829847097396851, Accuracy: 53.125%\n",
      "Epoch: 190, Loss: 0.6821914315223694, Accuracy: 53.125%\n",
      "Epoch: 191, Loss: 0.6821761131286621, Accuracy: 53.125%\n",
      "Epoch: 192, Loss: 0.6834339499473572, Accuracy: 53.125%\n",
      "Epoch: 193, Loss: 0.6824146509170532, Accuracy: 53.125%\n",
      "Epoch: 194, Loss: 0.6825019121170044, Accuracy: 53.125%\n",
      "Epoch: 195, Loss: 0.6837517619132996, Accuracy: 53.125%\n",
      "Epoch: 196, Loss: 0.6828607320785522, Accuracy: 53.125%\n",
      "Epoch: 197, Loss: 0.6832471489906311, Accuracy: 53.125%\n",
      "Epoch: 198, Loss: 0.6825121641159058, Accuracy: 53.125%\n",
      "Epoch: 199, Loss: 0.6822394728660583, Accuracy: 53.125%\n",
      "Epoch: 200, Loss: 0.6828599572181702, Accuracy: 53.125%\n",
      "Epoch: 201, Loss: 0.682847261428833, Accuracy: 53.125%\n",
      "Epoch: 202, Loss: 0.6824434995651245, Accuracy: 53.125%\n",
      "Epoch: 203, Loss: 0.6825631856918335, Accuracy: 53.125%\n",
      "Epoch: 204, Loss: 0.6824333667755127, Accuracy: 53.125%\n",
      "Epoch: 205, Loss: 0.6824772953987122, Accuracy: 53.125%\n",
      "Epoch: 206, Loss: 0.682992696762085, Accuracy: 53.125%\n",
      "Epoch: 207, Loss: 0.6831428408622742, Accuracy: 53.125%\n",
      "Epoch: 208, Loss: 0.6833027601242065, Accuracy: 53.125%\n",
      "Epoch: 209, Loss: 0.682558000087738, Accuracy: 53.125%\n",
      "Epoch: 210, Loss: 0.6827861070632935, Accuracy: 53.125%\n",
      "Epoch: 211, Loss: 0.6822839975357056, Accuracy: 53.125%\n",
      "Epoch: 212, Loss: 0.6826624870300293, Accuracy: 53.125%\n",
      "Epoch: 213, Loss: 0.6823090314865112, Accuracy: 53.125%\n",
      "Epoch: 214, Loss: 0.682479739189148, Accuracy: 53.125%\n",
      "Epoch: 215, Loss: 0.6827315092086792, Accuracy: 53.125%\n",
      "Epoch: 216, Loss: 0.6825792193412781, Accuracy: 53.125%\n",
      "Epoch: 217, Loss: 0.6823021173477173, Accuracy: 53.125%\n",
      "Epoch: 218, Loss: 0.6825677156448364, Accuracy: 53.125%\n",
      "Epoch: 219, Loss: 0.6827309131622314, Accuracy: 53.125%\n",
      "Epoch: 220, Loss: 0.682877242565155, Accuracy: 53.125%\n",
      "Epoch: 221, Loss: 0.6824108362197876, Accuracy: 53.125%\n",
      "Epoch: 222, Loss: 0.682908833026886, Accuracy: 53.125%\n",
      "Epoch: 223, Loss: 0.6824151277542114, Accuracy: 53.125%\n",
      "Epoch: 224, Loss: 0.6822963356971741, Accuracy: 53.125%\n",
      "Epoch: 225, Loss: 0.6827823519706726, Accuracy: 53.125%\n",
      "Epoch: 226, Loss: 0.6827813982963562, Accuracy: 53.125%\n",
      "Epoch: 227, Loss: 0.6821757555007935, Accuracy: 53.125%\n",
      "Epoch: 228, Loss: 0.6825355291366577, Accuracy: 53.125%\n",
      "Epoch: 229, Loss: 0.6819964647293091, Accuracy: 53.125%\n",
      "Epoch: 230, Loss: 0.6824588179588318, Accuracy: 53.125%\n",
      "Epoch: 231, Loss: 0.682506799697876, Accuracy: 53.125%\n",
      "Epoch: 232, Loss: 0.6827619671821594, Accuracy: 53.125%\n",
      "Epoch: 233, Loss: 0.6825765371322632, Accuracy: 53.125%\n",
      "Epoch: 234, Loss: 0.6821469068527222, Accuracy: 53.125%\n",
      "Epoch: 235, Loss: 0.6820858120918274, Accuracy: 53.125%\n",
      "Epoch: 236, Loss: 0.6824367642402649, Accuracy: 53.125%\n",
      "Epoch: 237, Loss: 0.682222843170166, Accuracy: 53.125%\n",
      "Epoch: 238, Loss: 0.6821765899658203, Accuracy: 53.125%\n",
      "Epoch: 239, Loss: 0.6824685335159302, Accuracy: 53.125%\n",
      "Epoch: 240, Loss: 0.6825734376907349, Accuracy: 53.125%\n",
      "Epoch: 241, Loss: 0.682034969329834, Accuracy: 53.125%\n",
      "Epoch: 242, Loss: 0.682396650314331, Accuracy: 53.125%\n",
      "Epoch: 243, Loss: 0.6823760271072388, Accuracy: 53.125%\n",
      "Epoch: 244, Loss: 0.6815717816352844, Accuracy: 53.125%\n",
      "Epoch: 245, Loss: 0.6826610565185547, Accuracy: 53.125%\n",
      "Epoch: 246, Loss: 0.6825152039527893, Accuracy: 53.125%\n",
      "Epoch: 247, Loss: 0.6830726861953735, Accuracy: 53.125%\n",
      "Epoch: 248, Loss: 0.6828698515892029, Accuracy: 53.125%\n",
      "Epoch: 249, Loss: 0.682367742061615, Accuracy: 53.125%\n",
      "Epoch: 250, Loss: 0.6820697784423828, Accuracy: 53.125%\n",
      "Epoch: 251, Loss: 0.6829363107681274, Accuracy: 53.125%\n",
      "Epoch: 252, Loss: 0.6825376152992249, Accuracy: 53.125%\n",
      "Epoch: 253, Loss: 0.6825606226921082, Accuracy: 53.125%\n",
      "Epoch: 254, Loss: 0.68267422914505, Accuracy: 53.125%\n",
      "Epoch: 255, Loss: 0.6835523843765259, Accuracy: 53.125%\n",
      "Epoch: 256, Loss: 0.6825070381164551, Accuracy: 53.125%\n",
      "Epoch: 257, Loss: 0.6825271844863892, Accuracy: 53.125%\n",
      "Epoch: 258, Loss: 0.6830264925956726, Accuracy: 53.125%\n",
      "Epoch: 259, Loss: 0.6829560399055481, Accuracy: 53.125%\n",
      "Epoch: 260, Loss: 0.6822367310523987, Accuracy: 53.125%\n",
      "Epoch: 261, Loss: 0.682553768157959, Accuracy: 53.125%\n",
      "Epoch: 262, Loss: 0.68272864818573, Accuracy: 53.125%\n",
      "Epoch: 263, Loss: 0.6822048425674438, Accuracy: 53.125%\n",
      "Epoch: 264, Loss: 0.6822080612182617, Accuracy: 53.125%\n",
      "Epoch: 265, Loss: 0.68224036693573, Accuracy: 53.125%\n",
      "Epoch: 266, Loss: 0.6829627156257629, Accuracy: 53.125%\n",
      "Epoch: 267, Loss: 0.6825579404830933, Accuracy: 53.125%\n",
      "Epoch: 268, Loss: 0.6826896667480469, Accuracy: 53.125%\n",
      "Epoch: 269, Loss: 0.6824480295181274, Accuracy: 53.125%\n",
      "Epoch: 270, Loss: 0.6823552250862122, Accuracy: 53.125%\n",
      "Epoch: 271, Loss: 0.6827360987663269, Accuracy: 53.125%\n",
      "Epoch: 272, Loss: 0.6821260452270508, Accuracy: 53.125%\n",
      "Epoch: 273, Loss: 0.6828137040138245, Accuracy: 53.125%\n",
      "Epoch: 274, Loss: 0.6822913885116577, Accuracy: 53.125%\n",
      "Epoch: 275, Loss: 0.6822777986526489, Accuracy: 53.125%\n",
      "Epoch: 276, Loss: 0.6828097105026245, Accuracy: 53.125%\n",
      "Epoch: 277, Loss: 0.6826416254043579, Accuracy: 53.125%\n",
      "Epoch: 278, Loss: 0.6824096441268921, Accuracy: 53.125%\n",
      "Epoch: 279, Loss: 0.6824266910552979, Accuracy: 53.125%\n",
      "Epoch: 280, Loss: 0.682254433631897, Accuracy: 53.125%\n",
      "Epoch: 281, Loss: 0.6823508739471436, Accuracy: 53.125%\n",
      "Epoch: 282, Loss: 0.6827030777931213, Accuracy: 53.125%\n",
      "Epoch: 283, Loss: 0.682601273059845, Accuracy: 53.125%\n",
      "Epoch: 284, Loss: 0.6821417212486267, Accuracy: 53.125%\n",
      "Epoch: 285, Loss: 0.6821866631507874, Accuracy: 53.125%\n",
      "Epoch: 286, Loss: 0.6828113794326782, Accuracy: 53.125%\n",
      "Epoch: 287, Loss: 0.6822798848152161, Accuracy: 53.125%\n",
      "Epoch: 288, Loss: 0.6823055148124695, Accuracy: 53.125%\n",
      "Epoch: 289, Loss: 0.6826740503311157, Accuracy: 53.125%\n",
      "Epoch: 290, Loss: 0.6822835206985474, Accuracy: 53.125%\n",
      "Epoch: 291, Loss: 0.6826429963111877, Accuracy: 53.125%\n",
      "Epoch: 292, Loss: 0.6824281811714172, Accuracy: 53.125%\n",
      "Epoch: 293, Loss: 0.6826406121253967, Accuracy: 53.125%\n",
      "Epoch: 294, Loss: 0.6826876401901245, Accuracy: 53.125%\n",
      "Epoch: 295, Loss: 0.6819337010383606, Accuracy: 53.125%\n",
      "Epoch: 296, Loss: 0.682550847530365, Accuracy: 53.125%\n",
      "Epoch: 297, Loss: 0.6827467679977417, Accuracy: 53.125%\n",
      "Epoch: 298, Loss: 0.6827865839004517, Accuracy: 53.125%\n",
      "Epoch: 299, Loss: 0.6822205781936646, Accuracy: 53.125%\n",
      "Epoch: 300, Loss: 0.6822586059570312, Accuracy: 53.125%\n",
      "Epoch: 301, Loss: 0.6821253895759583, Accuracy: 53.125%\n",
      "Epoch: 302, Loss: 0.6823753118515015, Accuracy: 53.125%\n",
      "Epoch: 303, Loss: 0.6831809282302856, Accuracy: 53.125%\n",
      "Epoch: 304, Loss: 0.6824346780776978, Accuracy: 53.125%\n",
      "Epoch: 305, Loss: 0.6821361780166626, Accuracy: 53.125%\n",
      "Epoch: 306, Loss: 0.6819735765457153, Accuracy: 53.125%\n",
      "Epoch: 307, Loss: 0.6821848154067993, Accuracy: 53.125%\n",
      "Epoch: 308, Loss: 0.6825028657913208, Accuracy: 53.125%\n",
      "Epoch: 309, Loss: 0.6823290586471558, Accuracy: 53.125%\n",
      "Epoch: 310, Loss: 0.6823744177818298, Accuracy: 53.125%\n",
      "Epoch: 311, Loss: 0.6826027035713196, Accuracy: 53.125%\n",
      "Epoch: 312, Loss: 0.6818158030509949, Accuracy: 53.125%\n",
      "Epoch: 313, Loss: 0.6825140714645386, Accuracy: 53.125%\n",
      "Epoch: 314, Loss: 0.6829816102981567, Accuracy: 53.125%\n",
      "Epoch: 315, Loss: 0.6817724704742432, Accuracy: 53.125%\n",
      "Epoch: 316, Loss: 0.6826233863830566, Accuracy: 53.125%\n",
      "Epoch: 317, Loss: 0.6824778318405151, Accuracy: 53.125%\n",
      "Epoch: 318, Loss: 0.6818349957466125, Accuracy: 53.125%\n",
      "Epoch: 319, Loss: 0.6824247241020203, Accuracy: 53.125%\n",
      "Epoch: 320, Loss: 0.6828877329826355, Accuracy: 53.125%\n",
      "Epoch: 321, Loss: 0.6830013394355774, Accuracy: 53.125%\n",
      "Epoch: 322, Loss: 0.6824368238449097, Accuracy: 53.125%\n",
      "Epoch: 323, Loss: 0.6828283071517944, Accuracy: 53.125%\n",
      "Epoch: 324, Loss: 0.6823127269744873, Accuracy: 53.125%\n",
      "Epoch: 325, Loss: 0.6830591559410095, Accuracy: 53.125%\n",
      "Epoch: 326, Loss: 0.681998074054718, Accuracy: 53.125%\n",
      "Epoch: 327, Loss: 0.6818801164627075, Accuracy: 53.125%\n",
      "Epoch: 328, Loss: 0.6827974319458008, Accuracy: 53.125%\n",
      "Epoch: 329, Loss: 0.6825142502784729, Accuracy: 53.125%\n",
      "Epoch: 330, Loss: 0.6823725700378418, Accuracy: 53.125%\n",
      "Epoch: 331, Loss: 0.6822186708450317, Accuracy: 53.125%\n",
      "Epoch: 332, Loss: 0.6830819249153137, Accuracy: 53.125%\n",
      "Epoch: 333, Loss: 0.6821703910827637, Accuracy: 53.125%\n",
      "Epoch: 334, Loss: 0.6826699376106262, Accuracy: 53.125%\n",
      "Epoch: 335, Loss: 0.6824194192886353, Accuracy: 53.125%\n",
      "Epoch: 336, Loss: 0.6831072568893433, Accuracy: 53.125%\n",
      "Epoch: 337, Loss: 0.6824878454208374, Accuracy: 53.125%\n",
      "Epoch: 338, Loss: 0.6822851896286011, Accuracy: 53.125%\n",
      "Epoch: 339, Loss: 0.6830921173095703, Accuracy: 53.125%\n",
      "Epoch: 340, Loss: 0.682705819606781, Accuracy: 53.125%\n",
      "Epoch: 341, Loss: 0.6828891634941101, Accuracy: 53.125%\n",
      "Epoch: 342, Loss: 0.6825309991836548, Accuracy: 53.125%\n",
      "Epoch: 343, Loss: 0.6819740533828735, Accuracy: 53.125%\n",
      "Epoch: 344, Loss: 0.6828116178512573, Accuracy: 53.125%\n",
      "Epoch: 345, Loss: 0.6819459795951843, Accuracy: 53.125%\n",
      "Epoch: 346, Loss: 0.682390034198761, Accuracy: 53.125%\n",
      "Epoch: 347, Loss: 0.6821030378341675, Accuracy: 53.125%\n",
      "Epoch: 348, Loss: 0.6825895309448242, Accuracy: 53.125%\n",
      "Epoch: 349, Loss: 0.6826910972595215, Accuracy: 53.125%\n",
      "Epoch: 350, Loss: 0.682931125164032, Accuracy: 53.125%\n",
      "Epoch: 351, Loss: 0.6824967861175537, Accuracy: 53.125%\n",
      "Epoch: 352, Loss: 0.6823398470878601, Accuracy: 53.125%\n",
      "Epoch: 353, Loss: 0.6826435327529907, Accuracy: 53.125%\n",
      "Epoch: 354, Loss: 0.6822558045387268, Accuracy: 53.125%\n",
      "Epoch: 355, Loss: 0.6825636625289917, Accuracy: 53.125%\n",
      "Epoch: 356, Loss: 0.6823363304138184, Accuracy: 53.125%\n",
      "Epoch: 357, Loss: 0.6823052763938904, Accuracy: 53.125%\n",
      "Epoch: 358, Loss: 0.6826509237289429, Accuracy: 53.125%\n",
      "Epoch: 359, Loss: 0.6822907328605652, Accuracy: 53.125%\n",
      "Epoch: 360, Loss: 0.682579755783081, Accuracy: 53.125%\n",
      "Epoch: 361, Loss: 0.682939887046814, Accuracy: 53.125%\n",
      "Epoch: 362, Loss: 0.682202935218811, Accuracy: 53.125%\n",
      "Epoch: 363, Loss: 0.6821226477622986, Accuracy: 53.125%\n",
      "Epoch: 364, Loss: 0.6822171211242676, Accuracy: 53.125%\n",
      "Epoch: 365, Loss: 0.6821576356887817, Accuracy: 53.125%\n",
      "Epoch: 366, Loss: 0.6825353503227234, Accuracy: 53.125%\n",
      "Epoch: 367, Loss: 0.682940661907196, Accuracy: 53.125%\n",
      "Epoch: 368, Loss: 0.6826532483100891, Accuracy: 53.125%\n",
      "Epoch: 369, Loss: 0.6826390027999878, Accuracy: 53.125%\n",
      "Epoch: 370, Loss: 0.6824432015419006, Accuracy: 53.125%\n",
      "Epoch: 371, Loss: 0.6822831034660339, Accuracy: 53.125%\n",
      "Epoch: 372, Loss: 0.6824403405189514, Accuracy: 53.125%\n",
      "Epoch: 373, Loss: 0.6825467348098755, Accuracy: 53.125%\n",
      "Epoch: 374, Loss: 0.6824939250946045, Accuracy: 53.125%\n",
      "Epoch: 375, Loss: 0.6823887228965759, Accuracy: 53.125%\n",
      "Epoch: 376, Loss: 0.682252824306488, Accuracy: 53.125%\n",
      "Epoch: 377, Loss: 0.6825835108757019, Accuracy: 53.125%\n",
      "Epoch: 378, Loss: 0.6823548078536987, Accuracy: 53.125%\n",
      "Epoch: 379, Loss: 0.682379961013794, Accuracy: 53.125%\n",
      "Epoch: 380, Loss: 0.6822738647460938, Accuracy: 53.125%\n",
      "Epoch: 381, Loss: 0.6826504468917847, Accuracy: 53.125%\n",
      "Epoch: 382, Loss: 0.6822267770767212, Accuracy: 53.125%\n",
      "Epoch: 383, Loss: 0.6821578145027161, Accuracy: 53.125%\n",
      "Epoch: 384, Loss: 0.6822271347045898, Accuracy: 53.125%\n",
      "Epoch: 385, Loss: 0.682414174079895, Accuracy: 53.125%\n",
      "Epoch: 386, Loss: 0.682302713394165, Accuracy: 53.125%\n",
      "Epoch: 387, Loss: 0.6823415756225586, Accuracy: 53.125%\n",
      "Epoch: 388, Loss: 0.6823217272758484, Accuracy: 53.125%\n",
      "Epoch: 389, Loss: 0.6823410391807556, Accuracy: 53.125%\n",
      "Epoch: 390, Loss: 0.6825711131095886, Accuracy: 53.125%\n",
      "Epoch: 391, Loss: 0.6826416254043579, Accuracy: 53.125%\n",
      "Epoch: 392, Loss: 0.6820491552352905, Accuracy: 53.125%\n",
      "Epoch: 393, Loss: 0.6823416352272034, Accuracy: 53.125%\n",
      "Epoch: 394, Loss: 0.6824069023132324, Accuracy: 53.125%\n",
      "Epoch: 395, Loss: 0.6826128959655762, Accuracy: 53.125%\n",
      "Epoch: 396, Loss: 0.6824239492416382, Accuracy: 53.125%\n",
      "Epoch: 397, Loss: 0.6823395490646362, Accuracy: 53.125%\n",
      "Epoch: 398, Loss: 0.6820847392082214, Accuracy: 53.125%\n",
      "Epoch: 399, Loss: 0.6819530725479126, Accuracy: 53.125%\n",
      "Epoch: 400, Loss: 0.682569146156311, Accuracy: 53.125%\n",
      "Epoch: 401, Loss: 0.6827297210693359, Accuracy: 53.125%\n",
      "Epoch: 402, Loss: 0.6820911169052124, Accuracy: 53.125%\n",
      "Epoch: 403, Loss: 0.6826571226119995, Accuracy: 53.125%\n",
      "Epoch: 404, Loss: 0.6823326349258423, Accuracy: 53.125%\n",
      "Epoch: 405, Loss: 0.6822497248649597, Accuracy: 53.125%\n",
      "Epoch: 406, Loss: 0.6825457811355591, Accuracy: 53.125%\n",
      "Epoch: 407, Loss: 0.6824139952659607, Accuracy: 53.125%\n",
      "Epoch: 408, Loss: 0.6825746893882751, Accuracy: 53.125%\n",
      "Epoch: 409, Loss: 0.6823266744613647, Accuracy: 53.125%\n",
      "Epoch: 410, Loss: 0.6825405359268188, Accuracy: 53.125%\n",
      "Epoch: 411, Loss: 0.6825125217437744, Accuracy: 53.125%\n",
      "Epoch: 412, Loss: 0.6819719672203064, Accuracy: 53.125%\n",
      "Epoch: 413, Loss: 0.6821674108505249, Accuracy: 53.125%\n",
      "Epoch: 414, Loss: 0.6822893619537354, Accuracy: 53.125%\n",
      "Epoch: 415, Loss: 0.6825485229492188, Accuracy: 53.125%\n",
      "Epoch: 416, Loss: 0.6827629804611206, Accuracy: 53.125%\n",
      "Epoch: 417, Loss: 0.6822680830955505, Accuracy: 53.125%\n",
      "Epoch: 418, Loss: 0.6817649006843567, Accuracy: 53.125%\n",
      "Epoch: 419, Loss: 0.6820867657661438, Accuracy: 53.125%\n",
      "Epoch: 420, Loss: 0.6823726892471313, Accuracy: 53.125%\n",
      "Epoch: 421, Loss: 0.68223637342453, Accuracy: 53.125%\n",
      "Epoch: 422, Loss: 0.6829923987388611, Accuracy: 53.125%\n",
      "Epoch: 423, Loss: 0.6824482083320618, Accuracy: 53.125%\n",
      "Epoch: 424, Loss: 0.6828017234802246, Accuracy: 53.125%\n",
      "Epoch: 425, Loss: 0.6820966005325317, Accuracy: 53.125%\n",
      "Epoch: 426, Loss: 0.6822964549064636, Accuracy: 53.125%\n",
      "Epoch: 427, Loss: 0.6827855110168457, Accuracy: 53.125%\n",
      "Epoch: 428, Loss: 0.6824989914894104, Accuracy: 53.125%\n",
      "Epoch: 429, Loss: 0.6822935342788696, Accuracy: 53.125%\n",
      "Epoch: 430, Loss: 0.6820265054702759, Accuracy: 53.125%\n",
      "Epoch: 431, Loss: 0.6823569536209106, Accuracy: 53.125%\n",
      "Epoch: 432, Loss: 0.6817675828933716, Accuracy: 53.125%\n",
      "Epoch: 433, Loss: 0.6826192140579224, Accuracy: 53.125%\n",
      "Epoch: 434, Loss: 0.6819990873336792, Accuracy: 53.125%\n",
      "Epoch: 435, Loss: 0.6821643114089966, Accuracy: 53.125%\n",
      "Epoch: 436, Loss: 0.6823651194572449, Accuracy: 53.125%\n",
      "Epoch: 437, Loss: 0.6825035214424133, Accuracy: 53.125%\n",
      "Epoch: 438, Loss: 0.6820577383041382, Accuracy: 53.125%\n",
      "Epoch: 439, Loss: 0.6821581721305847, Accuracy: 53.125%\n",
      "Epoch: 440, Loss: 0.6827850341796875, Accuracy: 53.125%\n",
      "Epoch: 441, Loss: 0.6824886798858643, Accuracy: 53.125%\n",
      "Epoch: 442, Loss: 0.6822576522827148, Accuracy: 53.125%\n",
      "Epoch: 443, Loss: 0.6820799112319946, Accuracy: 53.125%\n",
      "Epoch: 444, Loss: 0.682299017906189, Accuracy: 53.125%\n",
      "Epoch: 445, Loss: 0.6822683215141296, Accuracy: 53.125%\n",
      "Epoch: 446, Loss: 0.6822853684425354, Accuracy: 53.125%\n",
      "Epoch: 447, Loss: 0.6824123859405518, Accuracy: 53.125%\n",
      "Epoch: 448, Loss: 0.6827934980392456, Accuracy: 53.125%\n",
      "Epoch: 449, Loss: 0.6823848485946655, Accuracy: 53.125%\n",
      "Epoch: 450, Loss: 0.682369589805603, Accuracy: 53.125%\n",
      "Epoch: 451, Loss: 0.6818861961364746, Accuracy: 53.125%\n",
      "Epoch: 452, Loss: 0.6823427081108093, Accuracy: 53.125%\n",
      "Epoch: 453, Loss: 0.6829029321670532, Accuracy: 53.125%\n",
      "Epoch: 454, Loss: 0.6825252175331116, Accuracy: 53.125%\n",
      "Epoch: 455, Loss: 0.6821471452713013, Accuracy: 53.125%\n",
      "Epoch: 456, Loss: 0.682191014289856, Accuracy: 53.125%\n",
      "Epoch: 457, Loss: 0.6825708746910095, Accuracy: 53.125%\n",
      "Epoch: 458, Loss: 0.6826221346855164, Accuracy: 53.125%\n",
      "Epoch: 459, Loss: 0.6824174523353577, Accuracy: 53.125%\n",
      "Epoch: 460, Loss: 0.6821410655975342, Accuracy: 53.125%\n",
      "Epoch: 461, Loss: 0.6824700832366943, Accuracy: 53.125%\n",
      "Epoch: 462, Loss: 0.6822225451469421, Accuracy: 53.125%\n",
      "Epoch: 463, Loss: 0.6824240684509277, Accuracy: 53.125%\n",
      "Epoch: 464, Loss: 0.682288408279419, Accuracy: 53.125%\n",
      "Epoch: 465, Loss: 0.6820496320724487, Accuracy: 53.125%\n",
      "Epoch: 466, Loss: 0.6823288798332214, Accuracy: 53.125%\n",
      "Epoch: 467, Loss: 0.6821005940437317, Accuracy: 53.125%\n",
      "Epoch: 468, Loss: 0.6826022267341614, Accuracy: 53.125%\n",
      "Epoch: 469, Loss: 0.681990385055542, Accuracy: 53.125%\n",
      "Epoch: 470, Loss: 0.6821688413619995, Accuracy: 53.125%\n",
      "Epoch: 471, Loss: 0.6825170516967773, Accuracy: 53.125%\n",
      "Epoch: 472, Loss: 0.6820341944694519, Accuracy: 53.125%\n",
      "Epoch: 473, Loss: 0.6824252009391785, Accuracy: 53.125%\n",
      "Epoch: 474, Loss: 0.6827468872070312, Accuracy: 53.125%\n",
      "Epoch: 475, Loss: 0.6820032000541687, Accuracy: 53.125%\n",
      "Epoch: 476, Loss: 0.682409942150116, Accuracy: 53.125%\n",
      "Epoch: 477, Loss: 0.6822612285614014, Accuracy: 53.125%\n",
      "Epoch: 478, Loss: 0.6823240518569946, Accuracy: 53.125%\n",
      "Epoch: 479, Loss: 0.6822510957717896, Accuracy: 53.125%\n",
      "Epoch: 480, Loss: 0.6822015643119812, Accuracy: 53.125%\n",
      "Epoch: 481, Loss: 0.6818709373474121, Accuracy: 53.125%\n",
      "Epoch: 482, Loss: 0.6827180981636047, Accuracy: 53.125%\n",
      "Epoch: 483, Loss: 0.6824648976325989, Accuracy: 53.125%\n",
      "Epoch: 484, Loss: 0.681944727897644, Accuracy: 53.125%\n",
      "Epoch: 485, Loss: 0.6820699572563171, Accuracy: 53.125%\n",
      "Epoch: 486, Loss: 0.6818827390670776, Accuracy: 53.125%\n",
      "Epoch: 487, Loss: 0.6823290586471558, Accuracy: 53.125%\n",
      "Epoch: 488, Loss: 0.6831442713737488, Accuracy: 53.125%\n",
      "Epoch: 489, Loss: 0.6817880868911743, Accuracy: 53.125%\n",
      "Epoch: 490, Loss: 0.6823076009750366, Accuracy: 53.125%\n",
      "Epoch: 491, Loss: 0.6818903088569641, Accuracy: 53.125%\n",
      "Epoch: 492, Loss: 0.6822342872619629, Accuracy: 53.125%\n",
      "Epoch: 493, Loss: 0.6823013424873352, Accuracy: 53.125%\n",
      "Epoch: 494, Loss: 0.6827620267868042, Accuracy: 53.125%\n",
      "Epoch: 495, Loss: 0.6825529336929321, Accuracy: 53.125%\n",
      "Epoch: 496, Loss: 0.6824186444282532, Accuracy: 53.125%\n",
      "Epoch: 497, Loss: 0.6830903887748718, Accuracy: 53.125%\n",
      "Epoch: 498, Loss: 0.6822015643119812, Accuracy: 53.125%\n",
      "Epoch: 499, Loss: 0.6822614073753357, Accuracy: 53.125%\n",
      "Epoch: 500, Loss: 0.6827030181884766, Accuracy: 53.125%\n",
      "Epoch: 501, Loss: 0.6825226545333862, Accuracy: 53.125%\n",
      "Epoch: 502, Loss: 0.6817916631698608, Accuracy: 53.125%\n",
      "Epoch: 503, Loss: 0.6828184723854065, Accuracy: 53.125%\n",
      "Epoch: 504, Loss: 0.682233452796936, Accuracy: 53.125%\n",
      "Epoch: 505, Loss: 0.6826539039611816, Accuracy: 53.125%\n",
      "Epoch: 506, Loss: 0.6825788021087646, Accuracy: 53.125%\n",
      "Epoch: 507, Loss: 0.6823176741600037, Accuracy: 53.125%\n",
      "Epoch: 508, Loss: 0.6822599172592163, Accuracy: 53.125%\n",
      "Epoch: 509, Loss: 0.6821332573890686, Accuracy: 53.125%\n",
      "Epoch: 510, Loss: 0.6823784112930298, Accuracy: 53.125%\n",
      "Epoch: 511, Loss: 0.6824401617050171, Accuracy: 53.125%\n",
      "Epoch: 512, Loss: 0.6823103427886963, Accuracy: 53.125%\n",
      "Epoch: 513, Loss: 0.6824226975440979, Accuracy: 53.125%\n",
      "Epoch: 514, Loss: 0.6823062300682068, Accuracy: 53.125%\n",
      "Epoch: 515, Loss: 0.6825279593467712, Accuracy: 53.125%\n",
      "Epoch: 516, Loss: 0.6823496222496033, Accuracy: 53.125%\n",
      "Epoch: 517, Loss: 0.6825582385063171, Accuracy: 53.125%\n",
      "Epoch: 518, Loss: 0.6823465824127197, Accuracy: 53.125%\n",
      "Epoch: 519, Loss: 0.6820646524429321, Accuracy: 53.125%\n",
      "Epoch: 520, Loss: 0.6825525164604187, Accuracy: 53.125%\n",
      "Epoch: 521, Loss: 0.682699978351593, Accuracy: 53.125%\n",
      "Epoch: 522, Loss: 0.6824998259544373, Accuracy: 53.125%\n",
      "Epoch: 523, Loss: 0.6823623776435852, Accuracy: 53.125%\n",
      "Epoch: 524, Loss: 0.6823644638061523, Accuracy: 53.125%\n",
      "Epoch: 525, Loss: 0.6825408935546875, Accuracy: 53.125%\n",
      "Epoch: 526, Loss: 0.6825858354568481, Accuracy: 53.125%\n",
      "Epoch: 527, Loss: 0.6823383569717407, Accuracy: 53.125%\n",
      "Epoch: 528, Loss: 0.6820678114891052, Accuracy: 53.125%\n",
      "Epoch: 529, Loss: 0.6824880242347717, Accuracy: 53.125%\n",
      "Epoch: 530, Loss: 0.6825529336929321, Accuracy: 53.125%\n",
      "Epoch: 531, Loss: 0.6820496320724487, Accuracy: 53.125%\n",
      "Epoch: 532, Loss: 0.6826096773147583, Accuracy: 53.125%\n",
      "Epoch: 533, Loss: 0.6821663975715637, Accuracy: 53.125%\n",
      "Epoch: 534, Loss: 0.682212769985199, Accuracy: 53.125%\n",
      "Epoch: 535, Loss: 0.6824382543563843, Accuracy: 53.125%\n",
      "Epoch: 536, Loss: 0.6824334263801575, Accuracy: 53.125%\n",
      "Epoch: 537, Loss: 0.6823776960372925, Accuracy: 53.125%\n",
      "Epoch: 538, Loss: 0.6825078129768372, Accuracy: 53.125%\n",
      "Epoch: 539, Loss: 0.6823830008506775, Accuracy: 53.125%\n",
      "Epoch: 540, Loss: 0.6822527647018433, Accuracy: 53.125%\n",
      "Epoch: 541, Loss: 0.6822676658630371, Accuracy: 53.125%\n",
      "Epoch: 542, Loss: 0.6822400093078613, Accuracy: 53.125%\n",
      "Epoch: 543, Loss: 0.6823398470878601, Accuracy: 53.125%\n",
      "Epoch: 544, Loss: 0.6820636987686157, Accuracy: 53.125%\n",
      "Epoch: 545, Loss: 0.6825307607650757, Accuracy: 53.125%\n",
      "Epoch: 546, Loss: 0.6820527911186218, Accuracy: 53.125%\n",
      "Epoch: 547, Loss: 0.6823997497558594, Accuracy: 53.125%\n",
      "Epoch: 548, Loss: 0.6819325685501099, Accuracy: 53.125%\n",
      "Epoch: 549, Loss: 0.682477593421936, Accuracy: 53.125%\n",
      "Epoch: 550, Loss: 0.6823786497116089, Accuracy: 53.125%\n",
      "Epoch: 551, Loss: 0.6821677684783936, Accuracy: 53.125%\n",
      "Epoch: 552, Loss: 0.6822198629379272, Accuracy: 53.125%\n",
      "Epoch: 553, Loss: 0.6822582483291626, Accuracy: 53.125%\n",
      "Epoch: 554, Loss: 0.682601273059845, Accuracy: 53.125%\n",
      "Epoch: 555, Loss: 0.6824380159378052, Accuracy: 53.125%\n",
      "Epoch: 556, Loss: 0.6823693513870239, Accuracy: 53.125%\n",
      "Epoch: 557, Loss: 0.6821824908256531, Accuracy: 53.125%\n",
      "Epoch: 558, Loss: 0.6820446252822876, Accuracy: 53.125%\n",
      "Epoch: 559, Loss: 0.6822280287742615, Accuracy: 53.125%\n",
      "Epoch: 560, Loss: 0.6824654936790466, Accuracy: 53.125%\n",
      "Epoch: 561, Loss: 0.682547926902771, Accuracy: 53.125%\n",
      "Epoch: 562, Loss: 0.6825748682022095, Accuracy: 53.125%\n",
      "Epoch: 563, Loss: 0.6817125082015991, Accuracy: 53.125%\n",
      "Epoch: 564, Loss: 0.6821969151496887, Accuracy: 53.125%\n",
      "Epoch: 565, Loss: 0.6822460293769836, Accuracy: 53.125%\n",
      "Epoch: 566, Loss: 0.6824519634246826, Accuracy: 53.125%\n",
      "Epoch: 567, Loss: 0.68193519115448, Accuracy: 53.125%\n",
      "Epoch: 568, Loss: 0.6825658679008484, Accuracy: 53.125%\n",
      "Epoch: 569, Loss: 0.6821398138999939, Accuracy: 53.125%\n",
      "Epoch: 570, Loss: 0.6826634407043457, Accuracy: 53.125%\n",
      "Epoch: 571, Loss: 0.6823476552963257, Accuracy: 53.125%\n",
      "Epoch: 572, Loss: 0.6822220683097839, Accuracy: 53.125%\n",
      "Epoch: 573, Loss: 0.6824119687080383, Accuracy: 53.125%\n",
      "Epoch: 574, Loss: 0.6822298765182495, Accuracy: 53.125%\n",
      "Epoch: 575, Loss: 0.6825987696647644, Accuracy: 53.125%\n",
      "Epoch: 576, Loss: 0.6822869181632996, Accuracy: 53.125%\n",
      "Epoch: 577, Loss: 0.6826100945472717, Accuracy: 53.125%\n",
      "Epoch: 578, Loss: 0.6821849942207336, Accuracy: 53.125%\n",
      "Epoch: 579, Loss: 0.6822620630264282, Accuracy: 53.125%\n",
      "Epoch: 580, Loss: 0.6820918917655945, Accuracy: 53.125%\n",
      "Epoch: 581, Loss: 0.6818177103996277, Accuracy: 53.125%\n",
      "Epoch: 582, Loss: 0.682297945022583, Accuracy: 53.125%\n",
      "Epoch: 583, Loss: 0.6822298765182495, Accuracy: 53.125%\n",
      "Epoch: 584, Loss: 0.6825246810913086, Accuracy: 53.125%\n",
      "Epoch: 585, Loss: 0.6822894811630249, Accuracy: 53.125%\n",
      "Epoch: 586, Loss: 0.681942343711853, Accuracy: 53.125%\n",
      "Epoch: 587, Loss: 0.6825457811355591, Accuracy: 53.125%\n",
      "Epoch: 588, Loss: 0.68190997838974, Accuracy: 53.125%\n",
      "Epoch: 589, Loss: 0.6824599504470825, Accuracy: 53.125%\n",
      "Epoch: 590, Loss: 0.6821857690811157, Accuracy: 53.125%\n",
      "Epoch: 591, Loss: 0.6825821995735168, Accuracy: 53.125%\n",
      "Epoch: 592, Loss: 0.6820819973945618, Accuracy: 53.125%\n",
      "Epoch: 593, Loss: 0.6822003126144409, Accuracy: 53.125%\n",
      "Epoch: 594, Loss: 0.6816951632499695, Accuracy: 53.125%\n",
      "Epoch: 595, Loss: 0.6821926832199097, Accuracy: 53.125%\n",
      "Epoch: 596, Loss: 0.6827661991119385, Accuracy: 53.125%\n",
      "Epoch: 597, Loss: 0.6825064420700073, Accuracy: 53.125%\n",
      "Epoch: 598, Loss: 0.6824515461921692, Accuracy: 53.125%\n",
      "Epoch: 599, Loss: 0.6823799014091492, Accuracy: 53.125%\n",
      "Epoch: 600, Loss: 0.682416558265686, Accuracy: 53.125%\n",
      "Epoch: 601, Loss: 0.6825926899909973, Accuracy: 53.125%\n",
      "Epoch: 602, Loss: 0.6820806264877319, Accuracy: 53.125%\n",
      "Epoch: 603, Loss: 0.6828820109367371, Accuracy: 53.125%\n",
      "Epoch: 604, Loss: 0.6819705963134766, Accuracy: 53.125%\n",
      "Epoch: 605, Loss: 0.6824360489845276, Accuracy: 53.125%\n",
      "Epoch: 606, Loss: 0.6825181841850281, Accuracy: 53.125%\n",
      "Epoch: 607, Loss: 0.6826182007789612, Accuracy: 53.125%\n",
      "Epoch: 608, Loss: 0.6818132400512695, Accuracy: 53.125%\n",
      "Epoch: 609, Loss: 0.6823858022689819, Accuracy: 53.125%\n",
      "Epoch: 610, Loss: 0.682488203048706, Accuracy: 53.125%\n",
      "Epoch: 611, Loss: 0.682014524936676, Accuracy: 53.125%\n",
      "Epoch: 612, Loss: 0.6824698448181152, Accuracy: 53.125%\n",
      "Epoch: 613, Loss: 0.68287193775177, Accuracy: 53.125%\n",
      "Epoch: 614, Loss: 0.6825779676437378, Accuracy: 53.125%\n",
      "Epoch: 615, Loss: 0.682357132434845, Accuracy: 53.125%\n",
      "Epoch: 616, Loss: 0.6821430921554565, Accuracy: 53.125%\n",
      "Epoch: 617, Loss: 0.6826194524765015, Accuracy: 53.125%\n",
      "Epoch: 618, Loss: 0.6822686195373535, Accuracy: 53.125%\n",
      "Epoch: 619, Loss: 0.6828939318656921, Accuracy: 53.125%\n",
      "Epoch: 620, Loss: 0.6826860308647156, Accuracy: 53.125%\n",
      "Epoch: 621, Loss: 0.6825008988380432, Accuracy: 53.125%\n",
      "Epoch: 622, Loss: 0.6818400025367737, Accuracy: 53.125%\n",
      "Epoch: 623, Loss: 0.6823180913925171, Accuracy: 53.125%\n",
      "Epoch: 624, Loss: 0.682780385017395, Accuracy: 53.125%\n",
      "Epoch: 625, Loss: 0.682004988193512, Accuracy: 53.125%\n",
      "Epoch: 626, Loss: 0.6824639439582825, Accuracy: 53.125%\n",
      "Epoch: 627, Loss: 0.682574450969696, Accuracy: 53.125%\n",
      "Epoch: 628, Loss: 0.6821971535682678, Accuracy: 53.125%\n",
      "Epoch: 629, Loss: 0.6819965243339539, Accuracy: 53.125%\n",
      "Epoch: 630, Loss: 0.6823517084121704, Accuracy: 53.125%\n",
      "Epoch: 631, Loss: 0.6822741031646729, Accuracy: 53.125%\n",
      "Epoch: 632, Loss: 0.6822009086608887, Accuracy: 53.125%\n",
      "Epoch: 633, Loss: 0.6824744939804077, Accuracy: 53.125%\n",
      "Epoch: 634, Loss: 0.6821526288986206, Accuracy: 53.125%\n",
      "Epoch: 635, Loss: 0.6823574304580688, Accuracy: 53.125%\n",
      "Epoch: 636, Loss: 0.6824580430984497, Accuracy: 53.125%\n",
      "Epoch: 637, Loss: 0.6821867823600769, Accuracy: 53.125%\n",
      "Epoch: 638, Loss: 0.6824629902839661, Accuracy: 53.125%\n",
      "Epoch: 639, Loss: 0.6821019053459167, Accuracy: 53.125%\n",
      "Epoch: 640, Loss: 0.6822788119316101, Accuracy: 53.125%\n",
      "Epoch: 641, Loss: 0.6824434995651245, Accuracy: 53.125%\n",
      "Epoch: 642, Loss: 0.6820746064186096, Accuracy: 53.125%\n",
      "Epoch: 643, Loss: 0.6823713183403015, Accuracy: 53.125%\n",
      "Epoch: 644, Loss: 0.6820882558822632, Accuracy: 53.125%\n",
      "Epoch: 645, Loss: 0.6824116706848145, Accuracy: 53.125%\n",
      "Epoch: 646, Loss: 0.6823338270187378, Accuracy: 53.125%\n",
      "Epoch: 647, Loss: 0.6828168630599976, Accuracy: 53.125%\n",
      "Epoch: 648, Loss: 0.6826412081718445, Accuracy: 53.125%\n",
      "Epoch: 649, Loss: 0.682583212852478, Accuracy: 53.125%\n",
      "Epoch: 650, Loss: 0.6822326183319092, Accuracy: 53.125%\n",
      "Epoch: 651, Loss: 0.6824355125427246, Accuracy: 53.125%\n",
      "Epoch: 652, Loss: 0.6822827458381653, Accuracy: 53.125%\n",
      "Epoch: 653, Loss: 0.6820834875106812, Accuracy: 53.125%\n",
      "Epoch: 654, Loss: 0.6826028823852539, Accuracy: 53.125%\n",
      "Epoch: 655, Loss: 0.682380735874176, Accuracy: 53.125%\n",
      "Epoch: 656, Loss: 0.6823142766952515, Accuracy: 53.125%\n",
      "Epoch: 657, Loss: 0.681714653968811, Accuracy: 53.125%\n",
      "Epoch: 658, Loss: 0.6822139024734497, Accuracy: 53.125%\n",
      "Epoch: 659, Loss: 0.6825195550918579, Accuracy: 53.125%\n",
      "Epoch: 660, Loss: 0.6822110414505005, Accuracy: 53.125%\n",
      "Epoch: 661, Loss: 0.6820913553237915, Accuracy: 53.125%\n",
      "Epoch: 662, Loss: 0.682226300239563, Accuracy: 53.125%\n",
      "Epoch: 663, Loss: 0.6823217272758484, Accuracy: 53.125%\n",
      "Epoch: 664, Loss: 0.6823737025260925, Accuracy: 53.125%\n",
      "Epoch: 665, Loss: 0.6825929880142212, Accuracy: 53.125%\n",
      "Epoch: 666, Loss: 0.6824030876159668, Accuracy: 53.125%\n",
      "Epoch: 667, Loss: 0.6822977066040039, Accuracy: 53.125%\n",
      "Epoch: 668, Loss: 0.6824350357055664, Accuracy: 53.125%\n",
      "Epoch: 669, Loss: 0.6823376417160034, Accuracy: 53.125%\n",
      "Epoch: 670, Loss: 0.68250572681427, Accuracy: 53.125%\n",
      "Epoch: 671, Loss: 0.6822651624679565, Accuracy: 53.125%\n",
      "Epoch: 672, Loss: 0.6823176741600037, Accuracy: 53.125%\n",
      "Epoch: 673, Loss: 0.6822066307067871, Accuracy: 53.125%\n",
      "Epoch: 674, Loss: 0.6823514699935913, Accuracy: 53.125%\n",
      "Epoch: 675, Loss: 0.6821413040161133, Accuracy: 53.125%\n",
      "Epoch: 676, Loss: 0.6823718547821045, Accuracy: 53.125%\n",
      "Epoch: 677, Loss: 0.6822306513786316, Accuracy: 53.125%\n",
      "Epoch: 678, Loss: 0.6825612187385559, Accuracy: 53.125%\n",
      "Epoch: 679, Loss: 0.6823338270187378, Accuracy: 53.125%\n",
      "Epoch: 680, Loss: 0.6820353269577026, Accuracy: 53.125%\n",
      "Epoch: 681, Loss: 0.6823062896728516, Accuracy: 53.125%\n",
      "Epoch: 682, Loss: 0.6825660467147827, Accuracy: 53.125%\n",
      "Epoch: 683, Loss: 0.6821036338806152, Accuracy: 53.125%\n",
      "Epoch: 684, Loss: 0.6821015477180481, Accuracy: 53.125%\n",
      "Epoch: 685, Loss: 0.6824866533279419, Accuracy: 53.125%\n",
      "Epoch: 686, Loss: 0.6822227239608765, Accuracy: 53.125%\n",
      "Epoch: 687, Loss: 0.6824297308921814, Accuracy: 53.125%\n",
      "Epoch: 688, Loss: 0.6820988655090332, Accuracy: 53.125%\n",
      "Epoch: 689, Loss: 0.6823632717132568, Accuracy: 53.125%\n",
      "Epoch: 690, Loss: 0.6823188066482544, Accuracy: 53.125%\n",
      "Epoch: 691, Loss: 0.6819770336151123, Accuracy: 53.125%\n",
      "Epoch: 692, Loss: 0.682298481464386, Accuracy: 53.125%\n",
      "Epoch: 693, Loss: 0.6822511553764343, Accuracy: 53.125%\n",
      "Epoch: 694, Loss: 0.6826344728469849, Accuracy: 53.125%\n",
      "Epoch: 695, Loss: 0.6821832060813904, Accuracy: 53.125%\n",
      "Epoch: 696, Loss: 0.6823533177375793, Accuracy: 53.125%\n",
      "Epoch: 697, Loss: 0.6821683049201965, Accuracy: 53.125%\n",
      "Epoch: 698, Loss: 0.6821479797363281, Accuracy: 53.125%\n",
      "Epoch: 699, Loss: 0.6823849678039551, Accuracy: 53.125%\n",
      "Epoch: 700, Loss: 0.6820236444473267, Accuracy: 53.125%\n",
      "Epoch: 701, Loss: 0.6822273135185242, Accuracy: 53.125%\n",
      "Epoch: 702, Loss: 0.682637631893158, Accuracy: 53.125%\n",
      "Epoch: 703, Loss: 0.6824226379394531, Accuracy: 53.125%\n",
      "Epoch: 704, Loss: 0.6821869015693665, Accuracy: 53.125%\n",
      "Epoch: 705, Loss: 0.6822676658630371, Accuracy: 53.125%\n",
      "Epoch: 706, Loss: 0.6822395324707031, Accuracy: 53.125%\n",
      "Epoch: 707, Loss: 0.6821950674057007, Accuracy: 53.125%\n",
      "Epoch: 708, Loss: 0.6823074817657471, Accuracy: 53.125%\n",
      "Epoch: 709, Loss: 0.6822507381439209, Accuracy: 53.125%\n",
      "Epoch: 710, Loss: 0.682437002658844, Accuracy: 53.125%\n",
      "Epoch: 711, Loss: 0.6822740435600281, Accuracy: 53.125%\n",
      "Epoch: 712, Loss: 0.6824838519096375, Accuracy: 53.125%\n",
      "Epoch: 713, Loss: 0.6820479035377502, Accuracy: 53.125%\n",
      "Epoch: 714, Loss: 0.6822935342788696, Accuracy: 53.125%\n",
      "Epoch: 715, Loss: 0.6824800968170166, Accuracy: 53.125%\n",
      "Epoch: 716, Loss: 0.6825879812240601, Accuracy: 53.125%\n",
      "Epoch: 717, Loss: 0.6822969317436218, Accuracy: 53.125%\n",
      "Epoch: 718, Loss: 0.6822081208229065, Accuracy: 53.125%\n",
      "Epoch: 719, Loss: 0.6825340986251831, Accuracy: 53.125%\n",
      "Epoch: 720, Loss: 0.6823146343231201, Accuracy: 53.125%\n",
      "Epoch: 721, Loss: 0.6821454763412476, Accuracy: 53.125%\n",
      "Epoch: 722, Loss: 0.6819215416908264, Accuracy: 53.125%\n",
      "Epoch: 723, Loss: 0.682201623916626, Accuracy: 53.125%\n",
      "Epoch: 724, Loss: 0.6821659803390503, Accuracy: 53.125%\n",
      "Epoch: 725, Loss: 0.6822021007537842, Accuracy: 53.125%\n",
      "Epoch: 726, Loss: 0.6821378469467163, Accuracy: 53.125%\n",
      "Epoch: 727, Loss: 0.6821954250335693, Accuracy: 53.125%\n",
      "Epoch: 728, Loss: 0.6823869943618774, Accuracy: 53.125%\n",
      "Epoch: 729, Loss: 0.6826165318489075, Accuracy: 53.125%\n",
      "Epoch: 730, Loss: 0.6821833252906799, Accuracy: 53.125%\n",
      "Epoch: 731, Loss: 0.6821261048316956, Accuracy: 53.125%\n",
      "Epoch: 732, Loss: 0.6824651956558228, Accuracy: 53.125%\n",
      "Epoch: 733, Loss: 0.6821412444114685, Accuracy: 53.125%\n",
      "Epoch: 734, Loss: 0.6825783252716064, Accuracy: 53.125%\n",
      "Epoch: 735, Loss: 0.6823115944862366, Accuracy: 53.125%\n",
      "Epoch: 736, Loss: 0.6823491454124451, Accuracy: 53.125%\n",
      "Epoch: 737, Loss: 0.6821827292442322, Accuracy: 53.125%\n",
      "Epoch: 738, Loss: 0.6823601126670837, Accuracy: 53.125%\n",
      "Epoch: 739, Loss: 0.6824756860733032, Accuracy: 53.125%\n",
      "Epoch: 740, Loss: 0.6825028657913208, Accuracy: 53.125%\n",
      "Epoch: 741, Loss: 0.6822691559791565, Accuracy: 53.125%\n",
      "Epoch: 742, Loss: 0.6823546886444092, Accuracy: 53.125%\n",
      "Epoch: 743, Loss: 0.682216227054596, Accuracy: 53.125%\n",
      "Epoch: 744, Loss: 0.6820987462997437, Accuracy: 53.125%\n",
      "Epoch: 745, Loss: 0.6824990510940552, Accuracy: 53.125%\n",
      "Epoch: 746, Loss: 0.681972324848175, Accuracy: 53.125%\n",
      "Epoch: 747, Loss: 0.6822882890701294, Accuracy: 53.125%\n",
      "Epoch: 748, Loss: 0.6821421384811401, Accuracy: 53.125%\n",
      "Epoch: 749, Loss: 0.682319164276123, Accuracy: 53.125%\n",
      "Epoch: 750, Loss: 0.6822742223739624, Accuracy: 53.125%\n",
      "Epoch: 751, Loss: 0.6823927164077759, Accuracy: 53.125%\n",
      "Epoch: 752, Loss: 0.6824880838394165, Accuracy: 53.125%\n",
      "Epoch: 753, Loss: 0.6822431683540344, Accuracy: 53.125%\n",
      "Epoch: 754, Loss: 0.6823460459709167, Accuracy: 53.125%\n",
      "Epoch: 755, Loss: 0.6820631623268127, Accuracy: 53.125%\n",
      "Epoch: 756, Loss: 0.6820003986358643, Accuracy: 53.125%\n",
      "Epoch: 757, Loss: 0.6821049451828003, Accuracy: 53.125%\n",
      "Epoch: 758, Loss: 0.6822263598442078, Accuracy: 53.125%\n",
      "Epoch: 759, Loss: 0.6821625232696533, Accuracy: 53.125%\n",
      "Epoch: 760, Loss: 0.6826736927032471, Accuracy: 53.125%\n",
      "Epoch: 761, Loss: 0.6818329095840454, Accuracy: 53.125%\n",
      "Epoch: 762, Loss: 0.6825345754623413, Accuracy: 53.125%\n",
      "Epoch: 763, Loss: 0.6823426485061646, Accuracy: 53.125%\n",
      "Epoch: 764, Loss: 0.6822108030319214, Accuracy: 53.125%\n",
      "Epoch: 765, Loss: 0.6822011470794678, Accuracy: 53.125%\n",
      "Epoch: 766, Loss: 0.6826547384262085, Accuracy: 53.125%\n",
      "Epoch: 767, Loss: 0.6824564933776855, Accuracy: 53.125%\n",
      "Epoch: 768, Loss: 0.6820259094238281, Accuracy: 53.125%\n",
      "Epoch: 769, Loss: 0.6821714043617249, Accuracy: 53.125%\n",
      "Epoch: 770, Loss: 0.6824448108673096, Accuracy: 53.125%\n",
      "Epoch: 771, Loss: 0.6821902394294739, Accuracy: 53.125%\n",
      "Epoch: 772, Loss: 0.6821874380111694, Accuracy: 53.125%\n",
      "Epoch: 773, Loss: 0.6821425557136536, Accuracy: 53.125%\n",
      "Epoch: 774, Loss: 0.6823700666427612, Accuracy: 53.125%\n",
      "Epoch: 775, Loss: 0.6821096539497375, Accuracy: 53.125%\n",
      "Epoch: 776, Loss: 0.6824404001235962, Accuracy: 53.125%\n",
      "Epoch: 777, Loss: 0.682583749294281, Accuracy: 53.125%\n",
      "Epoch: 778, Loss: 0.6826075315475464, Accuracy: 53.125%\n",
      "Epoch: 779, Loss: 0.6822735071182251, Accuracy: 53.125%\n",
      "Epoch: 780, Loss: 0.682181179523468, Accuracy: 53.125%\n",
      "Epoch: 781, Loss: 0.6822556853294373, Accuracy: 53.125%\n",
      "Epoch: 782, Loss: 0.6823206543922424, Accuracy: 53.125%\n",
      "Epoch: 783, Loss: 0.6820840835571289, Accuracy: 53.125%\n",
      "Epoch: 784, Loss: 0.6821073293685913, Accuracy: 53.125%\n",
      "Epoch: 785, Loss: 0.6820791363716125, Accuracy: 53.125%\n",
      "Epoch: 786, Loss: 0.6822547316551208, Accuracy: 53.125%\n",
      "Epoch: 787, Loss: 0.6826111674308777, Accuracy: 53.125%\n",
      "Epoch: 788, Loss: 0.6825434565544128, Accuracy: 53.125%\n",
      "Epoch: 789, Loss: 0.682577908039093, Accuracy: 53.125%\n",
      "Epoch: 790, Loss: 0.6825056076049805, Accuracy: 53.125%\n",
      "Epoch: 791, Loss: 0.6824412941932678, Accuracy: 53.125%\n",
      "Epoch: 792, Loss: 0.6825545430183411, Accuracy: 53.125%\n",
      "Epoch: 793, Loss: 0.6825502514839172, Accuracy: 53.125%\n",
      "Epoch: 794, Loss: 0.6821874380111694, Accuracy: 53.125%\n",
      "Epoch: 795, Loss: 0.6821351647377014, Accuracy: 53.125%\n",
      "Epoch: 796, Loss: 0.682131826877594, Accuracy: 53.125%\n",
      "Epoch: 797, Loss: 0.6823708415031433, Accuracy: 53.125%\n",
      "Epoch: 798, Loss: 0.6823149919509888, Accuracy: 53.125%\n",
      "Epoch: 799, Loss: 0.6822147369384766, Accuracy: 53.125%\n",
      "Epoch: 800, Loss: 0.6821378469467163, Accuracy: 53.125%\n",
      "Epoch: 801, Loss: 0.6823867559432983, Accuracy: 53.125%\n",
      "Epoch: 802, Loss: 0.6822522878646851, Accuracy: 53.125%\n",
      "Epoch: 803, Loss: 0.6822218298912048, Accuracy: 53.125%\n",
      "Epoch: 804, Loss: 0.6822716593742371, Accuracy: 53.125%\n",
      "Epoch: 805, Loss: 0.6823153495788574, Accuracy: 53.125%\n",
      "Epoch: 806, Loss: 0.6821407079696655, Accuracy: 53.125%\n",
      "Epoch: 807, Loss: 0.682176411151886, Accuracy: 53.125%\n",
      "Epoch: 808, Loss: 0.6820640563964844, Accuracy: 53.125%\n",
      "Epoch: 809, Loss: 0.6823758482933044, Accuracy: 53.125%\n",
      "Epoch: 810, Loss: 0.6823979616165161, Accuracy: 53.125%\n",
      "Epoch: 811, Loss: 0.6823378801345825, Accuracy: 53.125%\n",
      "Epoch: 812, Loss: 0.6823263764381409, Accuracy: 53.125%\n",
      "Epoch: 813, Loss: 0.6820648908615112, Accuracy: 53.125%\n",
      "Epoch: 814, Loss: 0.6826030015945435, Accuracy: 53.125%\n",
      "Epoch: 815, Loss: 0.6824445724487305, Accuracy: 53.125%\n",
      "Epoch: 816, Loss: 0.6824347376823425, Accuracy: 53.125%\n",
      "Epoch: 817, Loss: 0.6821792721748352, Accuracy: 53.125%\n",
      "Epoch: 818, Loss: 0.6822969317436218, Accuracy: 53.125%\n",
      "Epoch: 819, Loss: 0.6823596358299255, Accuracy: 53.125%\n",
      "Epoch: 820, Loss: 0.6821621656417847, Accuracy: 53.125%\n",
      "Epoch: 821, Loss: 0.6822864413261414, Accuracy: 53.125%\n",
      "Epoch: 822, Loss: 0.6821532845497131, Accuracy: 53.125%\n",
      "Epoch: 823, Loss: 0.6823144555091858, Accuracy: 53.125%\n",
      "Epoch: 824, Loss: 0.6823363900184631, Accuracy: 53.125%\n",
      "Epoch: 825, Loss: 0.6822842359542847, Accuracy: 53.125%\n",
      "Epoch: 826, Loss: 0.6822177171707153, Accuracy: 53.125%\n",
      "Epoch: 827, Loss: 0.6823005676269531, Accuracy: 53.125%\n",
      "Epoch: 828, Loss: 0.6821605563163757, Accuracy: 53.125%\n",
      "Epoch: 829, Loss: 0.6822333931922913, Accuracy: 53.125%\n",
      "Epoch: 830, Loss: 0.6821314096450806, Accuracy: 53.125%\n",
      "Epoch: 831, Loss: 0.6822616457939148, Accuracy: 53.125%\n",
      "Epoch: 832, Loss: 0.6822012662887573, Accuracy: 53.125%\n",
      "Epoch: 833, Loss: 0.6824500560760498, Accuracy: 53.125%\n",
      "Epoch: 834, Loss: 0.6823098659515381, Accuracy: 53.125%\n",
      "Epoch: 835, Loss: 0.6823182106018066, Accuracy: 53.125%\n",
      "Epoch: 836, Loss: 0.6821786165237427, Accuracy: 53.125%\n",
      "Epoch: 837, Loss: 0.6824194192886353, Accuracy: 53.125%\n",
      "Epoch: 838, Loss: 0.6822999119758606, Accuracy: 53.125%\n",
      "Epoch: 839, Loss: 0.6824568510055542, Accuracy: 53.125%\n",
      "Epoch: 840, Loss: 0.6822490692138672, Accuracy: 53.125%\n",
      "Epoch: 841, Loss: 0.6824303865432739, Accuracy: 53.125%\n",
      "Epoch: 842, Loss: 0.6824198961257935, Accuracy: 53.125%\n",
      "Epoch: 843, Loss: 0.6822429895401001, Accuracy: 53.125%\n",
      "Epoch: 844, Loss: 0.6821771860122681, Accuracy: 53.125%\n",
      "Epoch: 845, Loss: 0.6821169257164001, Accuracy: 53.125%\n",
      "Epoch: 846, Loss: 0.6823571920394897, Accuracy: 53.125%\n",
      "Epoch: 847, Loss: 0.6823214292526245, Accuracy: 53.125%\n",
      "Epoch: 848, Loss: 0.6822425127029419, Accuracy: 53.125%\n",
      "Epoch: 849, Loss: 0.6822086572647095, Accuracy: 53.125%\n",
      "Epoch: 850, Loss: 0.6821384429931641, Accuracy: 53.125%\n",
      "Epoch: 851, Loss: 0.6822227239608765, Accuracy: 53.125%\n",
      "Epoch: 852, Loss: 0.6823009252548218, Accuracy: 53.125%\n",
      "Epoch: 853, Loss: 0.682249903678894, Accuracy: 53.125%\n",
      "Epoch: 854, Loss: 0.6822786331176758, Accuracy: 53.125%\n",
      "Epoch: 855, Loss: 0.6822210550308228, Accuracy: 53.125%\n",
      "Epoch: 856, Loss: 0.6823492050170898, Accuracy: 53.125%\n",
      "Epoch: 857, Loss: 0.6822366714477539, Accuracy: 53.125%\n",
      "Epoch: 858, Loss: 0.6822710633277893, Accuracy: 53.125%\n",
      "Epoch: 859, Loss: 0.6821581721305847, Accuracy: 53.125%\n",
      "Epoch: 860, Loss: 0.6822270154953003, Accuracy: 53.125%\n",
      "Epoch: 861, Loss: 0.6823137998580933, Accuracy: 53.125%\n",
      "Epoch: 862, Loss: 0.6822377443313599, Accuracy: 53.125%\n",
      "Epoch: 863, Loss: 0.682259738445282, Accuracy: 53.125%\n",
      "Epoch: 864, Loss: 0.6823655366897583, Accuracy: 53.125%\n",
      "Epoch: 865, Loss: 0.6822472810745239, Accuracy: 53.125%\n",
      "Epoch: 866, Loss: 0.6821801662445068, Accuracy: 53.125%\n",
      "Epoch: 867, Loss: 0.6823625564575195, Accuracy: 53.125%\n",
      "Epoch: 868, Loss: 0.6824691295623779, Accuracy: 53.125%\n",
      "Epoch: 869, Loss: 0.682355523109436, Accuracy: 53.125%\n",
      "Epoch: 870, Loss: 0.6821127533912659, Accuracy: 53.125%\n",
      "Epoch: 871, Loss: 0.6823236346244812, Accuracy: 53.125%\n",
      "Epoch: 872, Loss: 0.6822408437728882, Accuracy: 53.125%\n",
      "Epoch: 873, Loss: 0.6821911931037903, Accuracy: 53.125%\n",
      "Epoch: 874, Loss: 0.6820632219314575, Accuracy: 53.125%\n",
      "Epoch: 875, Loss: 0.6822810173034668, Accuracy: 53.125%\n",
      "Epoch: 876, Loss: 0.6821052432060242, Accuracy: 53.125%\n",
      "Epoch: 877, Loss: 0.6823191046714783, Accuracy: 53.125%\n",
      "Epoch: 878, Loss: 0.6822874546051025, Accuracy: 53.125%\n",
      "Epoch: 879, Loss: 0.6822730898857117, Accuracy: 53.125%\n",
      "Epoch: 880, Loss: 0.6820765733718872, Accuracy: 53.125%\n",
      "Epoch: 881, Loss: 0.6822844743728638, Accuracy: 53.125%\n",
      "Epoch: 882, Loss: 0.6821869611740112, Accuracy: 53.125%\n",
      "Epoch: 883, Loss: 0.6821712851524353, Accuracy: 53.125%\n",
      "Epoch: 884, Loss: 0.6824425458908081, Accuracy: 53.125%\n",
      "Epoch: 885, Loss: 0.6822769045829773, Accuracy: 53.125%\n",
      "Epoch: 886, Loss: 0.6821863651275635, Accuracy: 53.125%\n",
      "Epoch: 887, Loss: 0.682167649269104, Accuracy: 53.125%\n",
      "Epoch: 888, Loss: 0.6823856234550476, Accuracy: 53.125%\n",
      "Epoch: 889, Loss: 0.6823989748954773, Accuracy: 53.125%\n",
      "Epoch: 890, Loss: 0.6823791861534119, Accuracy: 53.125%\n",
      "Epoch: 891, Loss: 0.6823534369468689, Accuracy: 53.125%\n",
      "Epoch: 892, Loss: 0.682174801826477, Accuracy: 53.125%\n",
      "Epoch: 893, Loss: 0.6823636889457703, Accuracy: 53.125%\n",
      "Epoch: 894, Loss: 0.6825257539749146, Accuracy: 53.125%\n",
      "Epoch: 895, Loss: 0.6823850870132446, Accuracy: 53.125%\n",
      "Epoch: 896, Loss: 0.6822859048843384, Accuracy: 53.125%\n",
      "Epoch: 897, Loss: 0.6822112798690796, Accuracy: 53.125%\n",
      "Epoch: 898, Loss: 0.6822668313980103, Accuracy: 53.125%\n",
      "Epoch: 899, Loss: 0.6822735667228699, Accuracy: 53.125%\n",
      "Epoch: 900, Loss: 0.6821295619010925, Accuracy: 53.125%\n",
      "Epoch: 901, Loss: 0.6822049021720886, Accuracy: 53.125%\n",
      "Epoch: 902, Loss: 0.6823073625564575, Accuracy: 53.125%\n",
      "Epoch: 903, Loss: 0.6823240518569946, Accuracy: 53.125%\n",
      "Epoch: 904, Loss: 0.6822132468223572, Accuracy: 53.125%\n",
      "Epoch: 905, Loss: 0.6820762753486633, Accuracy: 53.125%\n",
      "Epoch: 906, Loss: 0.6822997331619263, Accuracy: 53.125%\n",
      "Epoch: 907, Loss: 0.6821957230567932, Accuracy: 53.125%\n",
      "Epoch: 908, Loss: 0.6821684837341309, Accuracy: 53.125%\n",
      "Epoch: 909, Loss: 0.6824186444282532, Accuracy: 53.125%\n",
      "Epoch: 910, Loss: 0.6821436285972595, Accuracy: 53.125%\n",
      "Epoch: 911, Loss: 0.6823229193687439, Accuracy: 53.125%\n",
      "Epoch: 912, Loss: 0.6822748780250549, Accuracy: 53.125%\n",
      "Epoch: 913, Loss: 0.6823769807815552, Accuracy: 53.125%\n",
      "Epoch: 914, Loss: 0.6820194125175476, Accuracy: 53.125%\n",
      "Epoch: 915, Loss: 0.6823328137397766, Accuracy: 53.125%\n",
      "Epoch: 916, Loss: 0.6822428107261658, Accuracy: 53.125%\n",
      "Epoch: 917, Loss: 0.6822727918624878, Accuracy: 53.125%\n",
      "Epoch: 918, Loss: 0.6822782754898071, Accuracy: 53.125%\n",
      "Epoch: 919, Loss: 0.6823461651802063, Accuracy: 53.125%\n",
      "Epoch: 920, Loss: 0.6822213530540466, Accuracy: 53.125%\n",
      "Epoch: 921, Loss: 0.6824287176132202, Accuracy: 53.125%\n",
      "Epoch: 922, Loss: 0.6823456883430481, Accuracy: 53.125%\n",
      "Epoch: 923, Loss: 0.6822636127471924, Accuracy: 53.125%\n",
      "Epoch: 924, Loss: 0.6821708083152771, Accuracy: 53.125%\n",
      "Epoch: 925, Loss: 0.6823871731758118, Accuracy: 53.125%\n",
      "Epoch: 926, Loss: 0.6821975708007812, Accuracy: 53.125%\n",
      "Epoch: 927, Loss: 0.6822260618209839, Accuracy: 53.125%\n",
      "Epoch: 928, Loss: 0.6822923421859741, Accuracy: 53.125%\n",
      "Epoch: 929, Loss: 0.6822899580001831, Accuracy: 53.125%\n",
      "Epoch: 930, Loss: 0.6823433041572571, Accuracy: 53.125%\n",
      "Epoch: 931, Loss: 0.6821430325508118, Accuracy: 53.125%\n",
      "Epoch: 932, Loss: 0.682357907295227, Accuracy: 53.125%\n",
      "Epoch: 933, Loss: 0.6822220683097839, Accuracy: 53.125%\n",
      "Epoch: 934, Loss: 0.6822412610054016, Accuracy: 53.125%\n",
      "Epoch: 935, Loss: 0.6822952628135681, Accuracy: 53.125%\n",
      "Epoch: 936, Loss: 0.6928644180297852, Accuracy: 53.125%\n",
      "Epoch: 937, Loss: 0.6890849471092224, Accuracy: 53.125%\n",
      "Epoch: 938, Loss: 0.6912800669670105, Accuracy: 53.125%\n",
      "Epoch: 939, Loss: 0.6841055750846863, Accuracy: 53.125%\n",
      "Epoch: 940, Loss: 0.686095118522644, Accuracy: 53.125%\n",
      "Epoch: 941, Loss: 0.6887276768684387, Accuracy: 53.125%\n",
      "Epoch: 942, Loss: 0.6825428605079651, Accuracy: 53.125%\n",
      "Epoch: 943, Loss: 0.6844878792762756, Accuracy: 53.125%\n",
      "Epoch: 944, Loss: 0.686772882938385, Accuracy: 53.125%\n",
      "Epoch: 945, Loss: 0.6829596757888794, Accuracy: 53.125%\n",
      "Epoch: 946, Loss: 0.6831602454185486, Accuracy: 53.125%\n",
      "Epoch: 947, Loss: 0.6850970983505249, Accuracy: 53.125%\n",
      "Epoch: 948, Loss: 0.6835644245147705, Accuracy: 53.125%\n",
      "Epoch: 949, Loss: 0.6826168298721313, Accuracy: 53.125%\n",
      "Epoch: 950, Loss: 0.6830987930297852, Accuracy: 53.125%\n",
      "Epoch: 951, Loss: 0.6842131614685059, Accuracy: 53.125%\n",
      "Epoch: 952, Loss: 0.6827181577682495, Accuracy: 53.125%\n",
      "Epoch: 953, Loss: 0.6826521158218384, Accuracy: 53.125%\n",
      "Epoch: 954, Loss: 0.683117687702179, Accuracy: 53.125%\n",
      "Epoch: 955, Loss: 0.6834924221038818, Accuracy: 53.125%\n",
      "Epoch: 956, Loss: 0.682476818561554, Accuracy: 53.125%\n",
      "Epoch: 957, Loss: 0.6824048161506653, Accuracy: 53.125%\n",
      "Epoch: 958, Loss: 0.6829428672790527, Accuracy: 53.125%\n",
      "Epoch: 959, Loss: 0.6831552386283875, Accuracy: 53.125%\n",
      "Epoch: 960, Loss: 0.6821262240409851, Accuracy: 53.125%\n",
      "Epoch: 961, Loss: 0.6824349164962769, Accuracy: 53.125%\n",
      "Epoch: 962, Loss: 0.6827337741851807, Accuracy: 53.125%\n",
      "Epoch: 963, Loss: 0.6827349066734314, Accuracy: 53.125%\n",
      "Epoch: 964, Loss: 0.682241678237915, Accuracy: 53.125%\n",
      "Epoch: 965, Loss: 0.6825284361839294, Accuracy: 53.125%\n",
      "Epoch: 966, Loss: 0.6824753284454346, Accuracy: 53.125%\n",
      "Epoch: 967, Loss: 0.6825650334358215, Accuracy: 53.125%\n",
      "Epoch: 968, Loss: 0.6824771761894226, Accuracy: 53.125%\n",
      "Epoch: 969, Loss: 0.6825257539749146, Accuracy: 53.125%\n",
      "Epoch: 970, Loss: 0.6825754642486572, Accuracy: 53.125%\n",
      "Epoch: 971, Loss: 0.6825779676437378, Accuracy: 53.125%\n",
      "Epoch: 972, Loss: 0.6824411153793335, Accuracy: 53.125%\n",
      "Epoch: 973, Loss: 0.682121217250824, Accuracy: 53.125%\n",
      "Epoch: 974, Loss: 0.6823786497116089, Accuracy: 53.125%\n",
      "Epoch: 975, Loss: 0.6822118759155273, Accuracy: 53.125%\n",
      "Epoch: 976, Loss: 0.6821924448013306, Accuracy: 53.125%\n",
      "Epoch: 977, Loss: 0.682135283946991, Accuracy: 53.125%\n",
      "Epoch: 978, Loss: 0.6822267770767212, Accuracy: 53.125%\n",
      "Epoch: 979, Loss: 0.6824148893356323, Accuracy: 53.125%\n",
      "Epoch: 980, Loss: 0.6824575066566467, Accuracy: 53.125%\n",
      "Epoch: 981, Loss: 0.6823259592056274, Accuracy: 53.125%\n",
      "Epoch: 982, Loss: 0.6822605729103088, Accuracy: 53.125%\n",
      "Epoch: 983, Loss: 0.6822410821914673, Accuracy: 53.125%\n",
      "Epoch: 984, Loss: 0.6820896863937378, Accuracy: 53.125%\n",
      "Epoch: 985, Loss: 0.682363748550415, Accuracy: 53.125%\n",
      "Epoch: 986, Loss: 0.6822776794433594, Accuracy: 53.125%\n",
      "Epoch: 987, Loss: 0.6821318864822388, Accuracy: 53.125%\n",
      "Epoch: 988, Loss: 0.6825137138366699, Accuracy: 53.125%\n",
      "Epoch: 989, Loss: 0.6821218132972717, Accuracy: 53.125%\n",
      "Epoch: 990, Loss: 0.6824522614479065, Accuracy: 53.125%\n",
      "Epoch: 991, Loss: 0.6825467348098755, Accuracy: 53.125%\n",
      "Epoch: 992, Loss: 0.6822338104248047, Accuracy: 53.125%\n",
      "Epoch: 993, Loss: 0.682254433631897, Accuracy: 53.125%\n",
      "Epoch: 994, Loss: 0.6825357675552368, Accuracy: 53.125%\n",
      "Epoch: 995, Loss: 0.6823042035102844, Accuracy: 53.125%\n",
      "Epoch: 996, Loss: 0.6823102235794067, Accuracy: 53.125%\n",
      "Epoch: 997, Loss: 0.6823052763938904, Accuracy: 53.125%\n",
      "Epoch: 998, Loss: 0.6821956038475037, Accuracy: 53.125%\n",
      "Epoch: 999, Loss: 0.6824299097061157, Accuracy: 53.125%\n",
      "Epoch: 1000, Loss: 0.6820546388626099, Accuracy: 53.125%\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(xtrain)\n",
    "    outputs = outputs.squeeze()\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 평가\n",
    "    outputs = model(xtest)\n",
    "    \n",
    "    # y_test를 one-hot 벡터로 변환\n",
    "    # 모델 출력과 정수로 된 레이블 비교\n",
    "    predicted_labels = outputs.argmax(dim=1)\n",
    "    accuracy = (predicted_labels == y_test).float().mean()\n",
    "\n",
    "    \n",
    "    # 모델 출력과 one-hot 레이블 비교\n",
    "    #accuracy = (outputs.argmax(dim=1) == y_one_hot.argmax(dim=1)).float().mean()\n",
    "    \n",
    "    print(f'Epoch: {epoch+1}, Loss: {loss.item()}, Accuracy: {accuracy.item()*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([39, 2]) torch.Size([640, 2])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
